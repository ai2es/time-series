{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import TensorDataset, Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from adam import Adam\n",
    "\n",
    "import tqdm\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from geckoml.data import load_data, transform_data, inv_transform_preds\n",
    "from geckoml.metrics import ensembled_metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/glade/work/schreck/repos/GECKO_OPT/dev/gecko-ml/config/toluene_agg.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = conf['species']\n",
    "data_path = conf['dir_path']\n",
    "aggregate_bins = conf['aggregate_bins']\n",
    "input_vars = conf['input_vars']\n",
    "output_vars = conf['output_vars']\n",
    "tendency_cols = conf['tendency_cols']\n",
    "log_trans_cols = conf['log_trans_cols']\n",
    "output_path = \"./\"\n",
    "scaler_type = conf['scaler_type']\n",
    "ensemble_members = conf[\"ensemble_members\"]\n",
    "seed = conf['random_seed']\n",
    "\n",
    "# Get the shapes of the input and output data \n",
    "input_size = len(input_vars)\n",
    "output_size = len(output_vars)\n",
    "\n",
    "start_time = 0\n",
    "num_timesteps = 1439\n",
    "batch_size = 16\n",
    "\n",
    "L1_penalty = 1.39e-5\n",
    "L2_penalty = 3.49e-4\n",
    "\n",
    "lr_patience = 3\n",
    "stopping_patience = 5\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(data_path, aggregate_bins, species, input_vars, output_vars, log_trans_cols)\n",
    "    \n",
    "transformed_data, x_scaler, y_scaler = transform_data(\n",
    "    data, \n",
    "    output_path, \n",
    "    species, \n",
    "    tendency_cols, \n",
    "    log_trans_cols,\n",
    "    scaler_type, \n",
    "    output_vars, \n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Batch the training data by experiment\n",
    "train_in_array = transformed_data['train_in'].copy()\n",
    "n_exps = len(train_in_array.index.unique(level='id'))\n",
    "n_timesteps = len(train_in_array.index.unique(level='Time [s]'))\n",
    "n_features = len(input_vars)\n",
    "out_col_idx = train_in_array.columns.get_indexer(output_vars)\n",
    "train_in_array = train_in_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "# Batch the validation data by experiment\n",
    "val_in_array = transformed_data['val_in'].copy()\n",
    "n_exps = len(val_in_array.index.unique(level='id'))\n",
    "n_timesteps = len(val_in_array.index.unique(level='Time [s]'))\n",
    "val_out_col_idx = val_in_array.columns.get_indexer(output_vars)\n",
    "val_in_array = val_in_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "train_out_array = transformed_data['train_out'].copy()\n",
    "n_exps = len(train_out_array.index.unique(level='id'))\n",
    "n_timesteps = len(train_out_array.index.unique(level='Time [s]'))\n",
    "n_features = len(output_vars)\n",
    "out_col_idx = train_out_array.columns.get_indexer(output_vars)\n",
    "train_out_array = train_out_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "val_out_array = transformed_data['val_out'].copy()\n",
    "n_exps = len(val_out_array.index.unique(level='id'))\n",
    "n_timesteps = len(val_out_array.index.unique(level='Time [s]'))\n",
    "val_out_col_idx = val_out_array.columns.get_indexer(output_vars)\n",
    "val_out_array = val_out_array.values.reshape(n_exps, n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TensorDataset(\n",
    "#     torch.from_numpy(transformed_data[\"train_in\"].copy().values).float(),\n",
    "#     torch.from_numpy(transformed_data[\"train_out\"].copy().values).float()\n",
    "# )\n",
    "train_data = TensorDataset(\n",
    "    torch.from_numpy(train_in_array).float(),\n",
    "    torch.from_numpy(train_out_array).float()\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "# valid_data = TensorDataset(\n",
    "#     torch.from_numpy(transformed_data[\"val_in\"].copy().values).float(),\n",
    "#     torch.from_numpy(transformed_data[\"val_out\"].copy().values).float()\n",
    "# )\n",
    "valid_data = TensorDataset(\n",
    "    torch.from_numpy(val_in_array).float(),\n",
    "    torch.from_numpy(val_out_array).float()\n",
    ")\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "valid_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "\n",
    "        self.scale = (1 / (in_channels*out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width, resize_input = 1024):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 2 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(10, self.width) # input channel is 2: (a(x), x)\n",
    "\n",
    "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "        \n",
    "        self.enlarge = nn.Linear(9, resize_input)\n",
    "        self.squish = nn.Linear(resize_input, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.enlarge(x).unsqueeze(-1)\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # x = x[..., :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.squish(x.reshape(x.shape[0], x.shape[1]))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x = shape[0], shape[1]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        return gridx.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ntrain = 1000\n",
    "# ntest = 100\n",
    "\n",
    "# sub = 2**3 #subsampling rate\n",
    "# h = 2**13 // sub #total grid size divided by the subsampling rate\n",
    "# s = h\n",
    "\n",
    "# batch_size = 20\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# epochs = 500\n",
    "# step_size = 50\n",
    "# gamma = 0.5\n",
    "\n",
    "modes = 16\n",
    "width = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = MatReader('data/burgers_data_R10.mat')\n",
    "# x_data = dataloader.read_field('a')[:,::sub]\n",
    "# y_data = dataloader.read_field('u')[:,::sub]\n",
    "\n",
    "# x_train = x_data[:ntrain,:]\n",
    "# y_train = y_data[:ntrain,:]\n",
    "# x_test = x_data[-ntest:,:]\n",
    "# y_test = y_data[-ntest:,:]\n",
    "\n",
    "# x_train = x_train.reshape(ntrain, s, 1)\n",
    "# x_test = x_test.reshape(ntest, s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550550\n"
     ]
    }
   ],
   "source": [
    "model = FNO1d(modes, width, resize_input = 16).to(device)\n",
    "print(count_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = L2_penalty)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=20, \n",
    "    eta_min=1e-2*learning_rate\n",
    ")\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, \n",
    "#         patience = lr_patience, \n",
    "#         verbose = True,\n",
    "#         min_lr = 1.0e-13\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "myloss = LpLoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 22.417957 train_mae 0.792602 val_loss 16.176359 val_mae 0.794645 lr 0.0002802747026301318\n",
      "Epoch 1 train_loss 16.924126 train_mae 0.791155 val_loss 15.005882 val_mae 0.794892 lr 0.00021404630011609955\n",
      "Epoch 2 train_loss 13.265509 train_mae 0.705045 val_loss 14.250486 val_mae 0.795082 lr 0.0009939057286000041\n",
      "Epoch 3 train_loss 13.613236 train_mae 0.754310 val_loss 11.543204 val_mae 0.795917 lr 0.00035203658779054726\n",
      "Epoch 4 train_loss 10.668476 train_mae 0.747207 val_loss 10.821776 val_mae 0.796190 lr 0.00015498214331173107\n",
      "Epoch 5 train_loss 11.307106 train_mae 0.849139 val_loss 10.533316 val_mae 0.796341 lr 0.0009757729755740051\n",
      "Epoch 6 train_loss 9.613633 train_mae 0.739867 val_loss 9.000327 val_mae 0.797142 lr 0.00042756493979441185\n",
      "Epoch 7 train_loss 7.795693 train_mae 0.702892 val_loss 8.451453 val_mae 0.797524 lr 0.00010453658778894987\n",
      "Epoch 8 train_loss 8.265524 train_mae 0.796532 val_loss 8.326717 val_mae 0.797596 lr 0.0009460482294830313\n",
      "Epoch 9 train_loss 8.687599 train_mae 0.833652 val_loss 7.303819 val_mae 0.798463 lr 0.000504999999989585\n",
      "Epoch 10 train_loss 6.591602 train_mae 0.736250 val_loss 6.829436 val_mae 0.798991 lr 6.395177052919877e-05\n",
      "Epoch 11 train_loss 6.691442 train_mae 0.782813 val_loss 6.772721 val_mae 0.799049 lr 0.0009054634122691825\n",
      "Epoch 12 train_loss 6.634548 train_mae 0.815906 val_loss 6.022323 val_mae 0.800038 lr 0.0005824350601833859\n",
      "Epoch 13 train_loss 6.497607 train_mae 0.849051 val_loss 5.580162 val_mae 0.800818 lr 3.422702443310155e-05\n",
      "Epoch 14 train_loss 5.698740 train_mae 0.818530 val_loss 5.555251 val_mae 0.800856 lr 0.0008550178567408569\n",
      "Epoch 15 train_loss 5.190488 train_mae 0.750523 val_loss 4.970680 val_mae 0.802214 lr 0.0006579634122034551\n",
      "Epoch 16 train_loss 4.681256 train_mae 0.731468 val_loss 4.558857 val_mae 0.803459 lr 1.6094271405437084e-05\n",
      "Epoch 17 train_loss 4.151311 train_mae 0.727162 val_loss 4.549122 val_mae 0.803472 lr 0.0007959536998620169\n",
      "Epoch 18 train_loss 4.634346 train_mae 0.818598 val_loss 4.094009 val_mae 0.805322 lr 0.0007297252974254422\n",
      "Epoch 19 train_loss 3.442372 train_mae 0.653446 val_loss 3.713006 val_mae 0.807513 lr 1e-05\n",
      "Epoch 20 train_loss 3.824131 train_mae 0.845247 val_loss 3.707981 val_mae 0.807565 lr 0.0007297252973559821\n",
      "Epoch 21 train_loss 3.856346 train_mae 0.855066 val_loss 3.352024 val_mae 0.810674 lr 0.0007959536999487752\n",
      "Epoch 22 train_loss 3.568107 train_mae 0.836714 val_loss 2.994485 val_mae 0.815688 lr 1.6094271405406804e-05\n",
      "Epoch 23 train_loss 3.315840 train_mae 0.892155 val_loss 2.987671 val_mae 0.815834 lr 0.0006579634122615719\n",
      "Epoch 24 train_loss 2.934843 train_mae 0.768181 val_loss 2.719851 val_mae 0.821641 lr 0.0008550178567576899\n",
      "Epoch 25 train_loss 2.648496 train_mae 0.850306 val_loss 2.404695 val_mae 0.833157 lr 3.4227024431457145e-05\n",
      "Epoch 26 train_loss 2.357300 train_mae 0.849009 val_loss 2.393014 val_mae 0.833740 lr 0.0005824350601851341\n",
      "Epoch 27 train_loss 2.219545 train_mae 0.769729 val_loss 2.204628 val_mae 0.845109 lr 0.0009054634123748798\n",
      "Epoch 28 train_loss 2.167759 train_mae 0.828810 val_loss 1.945372 val_mae 0.870420 lr 6.395177052626336e-05\n",
      "Epoch 29 train_loss 2.140958 train_mae 1.092731 val_loss 1.928561 val_mae 0.872604 lr 0.0005050000000397704\n",
      "Epoch 30 train_loss 1.861942 train_mae 0.784467 val_loss 1.804154 val_mae 0.891432 lr 0.0009460482293824589\n",
      "Epoch 31 train_loss 1.867877 train_mae 0.922163 val_loss 1.601268 val_mae 0.936224 lr 0.00010453658779238011\n",
      "Epoch 32 train_loss 1.675155 train_mae 0.942780 val_loss 1.581281 val_mae 0.941831 lr 0.0004275649398796245\n",
      "Epoch 33 train_loss 1.505633 train_mae 0.898071 val_loss 1.505607 val_mae 0.965753 lr 0.0009757729755666386\n",
      "Epoch 34 train_loss 1.676173 train_mae 1.092538 val_loss 1.357458 val_mae 1.028506 lr 0.00015498214331177698\n",
      "Epoch 35 train_loss 1.390635 train_mae 0.966119 val_loss 1.337190 val_mae 1.039288 lr 0.0003520365878092629\n",
      "Epoch 36 train_loss 1.327716 train_mae 0.991742 val_loss 1.294748 val_mae 1.063903 lr 0.0009939057286868782\n",
      "Epoch 37 train_loss 1.246948 train_mae 0.984792 val_loss 1.194187 val_mae 1.138510 lr 0.00021404630013334801\n",
      "Epoch 38 train_loss 1.187053 train_mae 1.134259 val_loss 1.176042 val_mae 1.155365 lr 0.00028027470263115\n",
      "Epoch 39 train_loss 1.176776 train_mae 1.149088 val_loss 1.154314 val_mae 1.177090 lr 0.0010000000000046784\n",
      "Epoch 40 train_loss 1.229703 train_mae 1.226074 val_loss 1.088790 val_mae 1.257750 lr 0.00028027470263510635\n",
      "Epoch 41 train_loss 1.094651 train_mae 1.295393 val_loss 1.073697 val_mae 1.280825 lr 0.00021404630013778725\n",
      "Epoch 42 train_loss 1.081954 train_mae 1.425257 val_loss 1.063249 val_mae 1.297948 lr 0.0009939057286938353\n",
      "Epoch 43 train_loss 1.063553 train_mae 1.326907 val_loss 1.022726 val_mae 1.377765 lr 0.000352036587823602\n",
      "Epoch 44 train_loss 1.029160 train_mae 1.404506 val_loss 1.011025 val_mae 1.406003 lr 0.00015498214331205928\n",
      "Epoch 45 train_loss 0.966728 train_mae 1.249918 val_loss 1.006366 val_mae 1.417733 lr 0.0009757729755758662\n",
      "Epoch 46 train_loss 1.054360 train_mae 1.470360 val_loss 0.981221 val_mae 1.490509 lr 0.000427564939891642\n",
      "Epoch 47 train_loss 0.990623 train_mae 1.537757 val_loss 0.972279 val_mae 1.521973 lr 0.0001045365877760626\n",
      "Epoch 48 train_loss 0.998936 train_mae 1.519494 val_loss 0.970277 val_mae 1.529298 lr 0.000946048229572192\n",
      "Epoch 49 train_loss 0.985803 train_mae 1.645892 val_loss 0.954871 val_mae 1.594398 lr 0.0005050000001488201\n",
      "Epoch 50 train_loss 0.954655 train_mae 1.616211 val_loss 0.948254 val_mae 1.628431 lr 6.39517705276586e-05\n",
      "Epoch 51 train_loss 0.960339 train_mae 1.638060 val_loss 0.947505 val_mae 1.632528 lr 0.000905463412396166\n",
      "Epoch 52 train_loss 0.891391 train_mae 1.391070 val_loss 0.938478 val_mae 1.687283 lr 0.0005824350602090788\n",
      "Epoch 53 train_loss 0.943059 train_mae 1.645812 val_loss 0.933758 val_mae 1.721713 lr 3.422702443708657e-05\n",
      "Epoch 54 train_loss 0.956445 train_mae 1.818180 val_loss 0.933514 val_mae 1.723668 lr 0.0008550178569390929\n",
      "Epoch 55 train_loss 0.945043 train_mae 1.731789 val_loss 0.927993 val_mae 1.768206 lr 0.0006579634122927604\n",
      "Epoch 56 train_loss 0.901024 train_mae 1.544804 val_loss 0.924579 val_mae 1.800643 lr 1.609427140715045e-05\n",
      "Epoch 57 train_loss 0.866367 train_mae 1.329788 val_loss 0.924510 val_mae 1.801388 lr 0.0007959536999042661\n",
      "Epoch 58 train_loss 0.911835 train_mae 1.684222 val_loss 0.921234 val_mae 1.833976 lr 0.0007297252975249413\n",
      "Epoch 59 train_loss 0.887573 train_mae 1.548005 val_loss 0.918763 val_mae 1.862099 lr 1e-05\n",
      "Epoch 60 train_loss 0.884439 train_mae 1.604589 val_loss 0.918729 val_mae 1.862398 lr 0.000729725297458829\n",
      "Epoch 61 train_loss 0.895722 train_mae 1.620188 val_loss 0.916620 val_mae 1.884811 lr 0.0007959536998368424\n",
      "Epoch 62 train_loss 0.935456 train_mae 2.061574 val_loss 0.914614 val_mae 1.907939 lr 1.6094271405406804e-05\n",
      "Epoch 63 train_loss 0.920131 train_mae 1.974096 val_loss 0.914574 val_mae 1.908344 lr 0.0006579634123578686\n",
      "Epoch 64 train_loss 0.934104 train_mae 2.068296 val_loss 0.913065 val_mae 1.924159 lr 0.0008550178567163649\n",
      "Epoch 65 train_loss 0.898867 train_mae 1.768309 val_loss 0.911276 val_mae 1.943021 lr 3.4227024428691095e-05\n",
      "Epoch 66 train_loss 0.920672 train_mae 1.957579 val_loss 0.911205 val_mae 1.943691 lr 0.0005824350603768559\n",
      "Epoch 67 train_loss 0.897688 train_mae 1.814709 val_loss 0.910063 val_mae 1.953805 lr 0.0009054634123404329\n",
      "Epoch 68 train_loss 0.900395 train_mae 1.882303 val_loss 0.908324 val_mae 1.967692 lr 6.395177052138542e-05\n",
      "Epoch 69 train_loss 0.911265 train_mae 2.028649 val_loss 0.908203 val_mae 1.968568 lr 0.0005050000000233079\n",
      "Epoch 70 train_loss 0.904628 train_mae 1.909028 val_loss 0.907240 val_mae 1.974775 lr 0.0009460482293384769\n",
      "Epoch 71 train_loss 0.898497 train_mae 1.862259 val_loss 0.905468 val_mae 1.985412 lr 0.00010453658779343811\n",
      "Epoch 72 train_loss 0.893558 train_mae 1.912103 val_loss 0.905281 val_mae 1.986469 lr 0.00042756493986652273\n",
      "Epoch 73 train_loss 0.903726 train_mae 1.903581 val_loss 0.904465 val_mae 1.989595 lr 0.0009757729755199158\n",
      "Epoch 74 train_loss 0.898620 train_mae 1.939574 val_loss 0.902612 val_mae 1.995480 lr 0.0001549821433818994\n",
      "Epoch 75 train_loss 0.935583 train_mae 2.318415 val_loss 0.902323 val_mae 1.996581 lr 0.00035203658786940416\n",
      "Epoch 76 train_loss 0.878370 train_mae 1.744728 val_loss 0.901638 val_mae 1.998053 lr 0.0009939057290085118\n",
      "Epoch 77 train_loss 0.896991 train_mae 1.934853 val_loss 0.899701 val_mae 2.001039 lr 0.00021404630015759975\n",
      "Epoch 78 train_loss 0.907780 train_mae 2.139431 val_loss 0.899278 val_mae 2.001613 lr 0.0002802747027111606\n",
      "Epoch 79 train_loss 0.903080 train_mae 2.013498 val_loss 0.898709 val_mae 2.002096 lr 0.0010000000005099458\n",
      "Epoch 80 train_loss 0.897005 train_mae 1.951953 val_loss 0.896641 val_mae 2.003320 lr 0.00028027470261182723\n",
      "Epoch 81 train_loss 0.920468 train_mae 2.192229 val_loss 0.896052 val_mae 2.003723 lr 0.0002140463001969728\n",
      "Epoch 82 train_loss 0.875513 train_mae 1.742081 val_loss 0.895590 val_mae 2.003353 lr 0.0009939057288329963\n",
      "Epoch 83 train_loss 0.884443 train_mae 1.784727 val_loss 0.893387 val_mae 1.999945 lr 0.0003520365876698964\n",
      "Epoch 84 train_loss 0.879696 train_mae 1.874751 val_loss 0.892591 val_mae 1.998198 lr 0.000154982143384833\n",
      "Epoch 85 train_loss 0.872375 train_mae 1.675371 val_loss 0.892233 val_mae 1.996949 lr 0.0009757729758941179\n",
      "Epoch 86 train_loss 0.900536 train_mae 2.098194 val_loss 0.889849 val_mae 1.988127 lr 0.00042756493972042286\n",
      "Epoch 87 train_loss 0.861264 train_mae 1.618401 val_loss 0.888781 val_mae 1.983222 lr 0.00010453658780581673\n",
      "Epoch 88 train_loss 0.889007 train_mae 1.983290 val_loss 0.888518 val_mae 1.982044 lr 0.0009460482291903054\n",
      "Epoch 89 train_loss 0.897973 train_mae 2.051839 val_loss 0.886066 val_mae 1.970365 lr 0.000504999999945554\n",
      "Epoch 90 train_loss 0.892552 train_mae 1.984220 val_loss 0.884727 val_mae 1.963802 lr 6.395177055097441e-05\n",
      "Epoch 91 train_loss 0.859779 train_mae 1.744756 val_loss 0.884550 val_mae 1.962781 lr 0.0009054634120243145\n",
      "Epoch 92 train_loss 0.882657 train_mae 1.945535 val_loss 0.882026 val_mae 1.947563 lr 0.0005824350599594946\n",
      "Epoch 93 train_loss 0.872534 train_mae 1.910905 val_loss 0.880340 val_mae 1.936901 lr 3.422702444443822e-05\n",
      "Epoch 94 train_loss 0.910133 train_mae 2.201978 val_loss 0.880240 val_mae 1.936313 lr 0.0008550178565777407\n",
      "Epoch 95 train_loss 0.877179 train_mae 1.908869 val_loss 0.877654 val_mae 1.920345 lr 0.000657963412265678\n",
      "Epoch 96 train_loss 0.888873 train_mae 2.041497 val_loss 0.875607 val_mae 1.907470 lr 1.609427140886382e-05\n",
      "Epoch 97 train_loss 0.879046 train_mae 1.862198 val_loss 0.875553 val_mae 1.907098 lr 0.000795953699577385\n",
      "Epoch 98 train_loss 0.866102 train_mae 1.849812 val_loss 0.872983 val_mae 1.890120 lr 0.0007297252972155862\n",
      "Epoch 99 train_loss 0.853726 train_mae 1.675935 val_loss 0.870608 val_mae 1.873706 lr 1e-05\n",
      "Epoch 100 train_loss 0.883097 train_mae 1.935088 val_loss 0.870575 val_mae 1.873462 lr 0.0007297252974382199\n",
      "Epoch 101 train_loss 0.877554 train_mae 1.859072 val_loss 0.868027 val_mae 1.855435 lr 0.0007959537000940471\n",
      "Epoch 102 train_loss 0.860116 train_mae 1.747570 val_loss 0.865135 val_mae 1.834369 lr 1.6094271405406804e-05\n",
      "Epoch 103 train_loss 0.852832 train_mae 1.678074 val_loss 0.865076 val_mae 1.833933 lr 0.0006579634120878874\n",
      "Epoch 104 train_loss 0.873983 train_mae 1.782849 val_loss 0.862587 val_mae 1.816401 lr 0.0008550178566903129\n",
      "Epoch 105 train_loss 0.855229 train_mae 1.756302 val_loss 0.859289 val_mae 1.792580 lr 3.4227024437109775e-05\n",
      "Epoch 106 train_loss 0.862353 train_mae 1.721758 val_loss 0.859157 val_mae 1.791630 lr 0.0005824350603640027\n",
      "Epoch 107 train_loss 0.824114 train_mae 1.497926 val_loss 0.856747 val_mae 1.771865 lr 0.0009054634119621441\n",
      "Epoch 108 train_loss 0.848129 train_mae 1.684436 val_loss 0.852894 val_mae 1.740810 lr 6.395177051323735e-05\n",
      "Epoch 109 train_loss 0.826402 train_mae 1.476032 val_loss 0.852623 val_mae 1.738659 lr 0.0005049999999996369\n",
      "Epoch 110 train_loss 0.860314 train_mae 1.803621 val_loss 0.850444 val_mae 1.721241 lr 0.0009460482296504611\n",
      "Epoch 111 train_loss 0.838808 train_mae 1.649042 val_loss 0.846144 val_mae 1.686570 lr 0.00010453658781675453\n",
      "Epoch 112 train_loss 0.822832 train_mae 1.383159 val_loss 0.845649 val_mae 1.682473 lr 0.000427564939677663\n",
      "Epoch 113 train_loss 0.822646 train_mae 1.493872 val_loss 0.843611 val_mae 1.665111 lr 0.0009757729754776361\n",
      "Epoch 114 train_loss 0.846418 train_mae 1.598241 val_loss 0.838820 val_mae 1.627162 lr 0.00015498214331289266\n",
      "Epoch 115 train_loss 0.812520 train_mae 1.346993 val_loss 0.838017 val_mae 1.620582 lr 0.00035203658785259875\n",
      "Epoch 116 train_loss 0.829745 train_mae 1.572064 val_loss 0.836135 val_mae 1.605666 lr 0.0009939057289606564\n",
      "Epoch 117 train_loss 0.799990 train_mae 1.246209 val_loss 0.830645 val_mae 1.563239 lr 0.00021404630007449573\n",
      "Epoch 118 train_loss 0.818739 train_mae 1.417399 val_loss 0.829415 val_mae 1.553706 lr 0.000280274702597694\n",
      "Epoch 119 train_loss 0.856088 train_mae 1.754200 val_loss 0.827764 val_mae 1.542303 lr 0.0010000000000999613\n",
      "Epoch 120 train_loss 0.830809 train_mae 1.518971 val_loss 0.821584 val_mae 1.502832 lr 0.0002802747027141696\n",
      "Epoch 121 train_loss 0.823143 train_mae 1.516755 val_loss 0.819782 val_mae 1.491920 lr 0.00021404630019818702\n",
      "Epoch 122 train_loss 0.822306 train_mae 1.328996 val_loss 0.818379 val_mae 1.483495 lr 0.0009939057284230112\n",
      "Epoch 123 train_loss 0.856030 train_mae 1.853253 val_loss 0.811600 val_mae 1.445917 lr 0.00035203658778286287\n",
      "Epoch 124 train_loss 0.841064 train_mae 1.678806 val_loss 0.809154 val_mae 1.433419 lr 0.00015498214334394472\n",
      "Epoch 125 train_loss 0.777151 train_mae 1.236795 val_loss 0.808023 val_mae 1.427423 lr 0.0009757729758507707\n",
      "Epoch 126 train_loss 0.807507 train_mae 1.390176 val_loss 0.800839 val_mae 1.387568 lr 0.00042756494001244897\n",
      "Epoch 127 train_loss 0.814644 train_mae 1.307636 val_loss 0.797615 val_mae 1.369820 lr 0.00010453658777411958\n",
      "Epoch 128 train_loss 0.793286 train_mae 1.167674 val_loss 0.796810 val_mae 1.365238 lr 0.0009460482294876408\n",
      "Epoch 129 train_loss 0.779891 train_mae 1.297562 val_loss 0.789242 val_mae 1.319421 lr 0.0005050000000939841\n",
      "Epoch 130 train_loss 0.790482 train_mae 1.295784 val_loss 0.785113 val_mae 1.295926 lr 6.395177054125892e-05\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    # Train in batch mode\n",
    "    fiter = list(range(transformed_data['train_in'].shape[0] // batch_size)) #\n",
    "    #fiter = tqdm.tqdm(range(transformed_data['train_in'].shape[0] // batch_size), leave=True)\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_mae = []\n",
    "    for k, t in enumerate(fiter):\n",
    "\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "\n",
    "        y_pred = model(x.to(device))\n",
    "        mae_loss = torch.nn.functional.l1_loss(y.to(device), y_pred)\n",
    "        loss = myloss(y.to(device), y_pred)\n",
    "        #l1_norm = sum(p.abs().sum() for p in model.parameters()).cpu()\n",
    "        #loss += L1_penalty * l1_norm\n",
    "        train_loss.append(loss.item())\n",
    "        train_mae.append(mae_loss.item())\n",
    "\n",
    "        #fiter.set_description(f\"loss {np.mean(train_loss):.4f}\")\n",
    "        #fiter.update()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (k + 1) % 1000:\n",
    "            break\n",
    "        \n",
    "        #lr_scheduler.step()\n",
    "        \n",
    "    # Validate \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Validate in batch mode\n",
    "        valid_loss = []\n",
    "        valid_mae = []\n",
    "        fiter = list(range(transformed_data['val_in'].shape[0] // batch_size))\n",
    "        for t in fiter:\n",
    "\n",
    "            try:\n",
    "                x, y = next(valid_iter)\n",
    "            except StopIteration:\n",
    "                valid_iter = iter(valid_loader)\n",
    "                x, y = next(valid_iter)\n",
    "            y_pred = model(x.to(device))\n",
    "            mae_loss = torch.nn.functional.l1_loss(y.to(device), y_pred)\n",
    "            loss = myloss(y.to(device), y_pred)\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_mae.append(mae_loss.item())\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "        \n",
    "#         # Validate in box mode\n",
    "#         box_loss = []\n",
    "#         box_nll = []\n",
    "#         box_mae = []\n",
    "        \n",
    "#         # set up array for saving predicted results\n",
    "#         _in_array = torch.from_numpy(val_in_array).float()#.to(device).float()\n",
    "#         pred_array = np.empty((val_in_array.shape[0], num_timesteps-start_time, len(out_col_idx)))\n",
    "\n",
    "#         # use initial condition @ t = start_time and get the first prediction\n",
    "#         print(_in_array[:, start_time, :].shape)\n",
    "#         output = model(_in_array[:, start_time, :].to(device))\n",
    "#         pred_array[:, 0, :] = output.cpu().numpy()\n",
    "#         loss = torch.nn.functional.l1_loss(_in_array[:, start_time + 1, out_col_idx], output.cpu()).item()\n",
    "#         ev_loss = myloss(_in_array[:, start_time + 1, out_col_idx], output.cpu())\n",
    "        \n",
    "#         box_loss.append(loss)\n",
    "#         box_nll.append(ev_loss)\n",
    "\n",
    "#         # use the first prediction to get the next, and so on for num_timesteps\n",
    "#         for k, i in enumerate(range(start_time + 1, num_timesteps)): \n",
    "#             new_input = _in_array[:, i, :]\n",
    "#             new_input[:, out_col_idx] = output.cpu()\n",
    "#             output = model(new_input.to(device))\n",
    "#             pred_array[:, k+1, :] = output.cpu().numpy()\n",
    "#             if i < (num_timesteps-1):\n",
    "#                 loss = torch.nn.functional.l1_loss(_in_array[:, i+1, out_col_idx], output.cpu()).item()\n",
    "#                 ev_loss = myloss(\n",
    "#                     _in_array[:, start_time + 1, out_col_idx],\n",
    "#                     output.cpu()\n",
    "#                 )\n",
    "#                 box_loss.append(loss)\n",
    "#                 box_nll.append(ev_loss)\n",
    "                \n",
    "#         idx = transformed_data[\"val_out\"].index\n",
    "#         start_time_units = sorted(list(set([x[0] for x in idx])))[start_time]\n",
    "#         start_time_condition = [(x[0] >= start_time_units) for x in idx]\n",
    "#         idx = transformed_data[\"val_out\"][start_time_condition].index\n",
    "\n",
    "#         raw_box_preds = pd.DataFrame(\n",
    "#             data=pred_array.reshape(-1, len(output_vars)),\n",
    "#             columns=output_vars, \n",
    "#             index=idx\n",
    "#         )\n",
    "\n",
    "#         # inverse transform \n",
    "#         truth, preds = inv_transform_preds(\n",
    "#             raw_preds=raw_box_preds,\n",
    "#             truth=data['val_out'][start_time_condition],\n",
    "#             y_scaler=y_scaler,\n",
    "#             log_trans_cols=log_trans_cols,\n",
    "#             tendency_cols=tendency_cols)\n",
    "                \n",
    "#         metrics = ensembled_metrics(y_true=truth,\n",
    "#                                     y_pred=preds,\n",
    "#                                     member=0,\n",
    "#                                     output_vars=output_vars,\n",
    "#                                     stability_thresh=1.0)\n",
    "#         mean_box_mae = metrics['mean_mae'].mean()\n",
    "#         unstable_exps = metrics['n_unstable'].mean()\n",
    "#         box_mae.append(mean_box_mae)\n",
    "        \n",
    "    results_dict[\"epoch\"].append(epoch)\n",
    "    results_dict[\"train_loss\"].append(np.mean(train_loss))\n",
    "    results_dict[\"train_mae\"].append(np.mean(train_mae))\n",
    "    results_dict[\"val_loss\"].append(np.mean(valid_loss))\n",
    "    results_dict[\"val_mae\"].append(np.mean(valid_mae))\n",
    "#     results_dict[\"box_loss\"].append(np.mean(box_nll))\n",
    "#     results_dict[\"box_step_mae\"].append(np.mean(box_loss))\n",
    "#     results_dict[\"box_mae\"].append(np.mean(box_mae))\n",
    "#     results_dict[\"n_unstable\"].append(unstable_exps)\n",
    "    results_dict[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save the dataframe to disk\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "    df.to_csv(f\"gecko/training_log.csv\", index = False)\n",
    "    \n",
    "    print(f'Epoch {epoch} train_loss {results_dict[\"train_loss\"][-1]:4f}',\n",
    "          f'train_mae {results_dict[\"train_mae\"][-1]:4f}',\n",
    "          f'val_loss {results_dict[\"val_loss\"][-1]:4f}',\n",
    "          f'val_mae {results_dict[\"val_mae\"][-1]:4f}',\n",
    "#           f'box_loss {results_dict[\"box_loss\"][-1]:4f}',\n",
    "#           f'box_step_mae {results_dict[\"box_step_mae\"][-1]:4f}',\n",
    "#           f'box_mae {results_dict[\"box_mae\"][-1]:4f}',\n",
    "#           f'n_unstable {int(results_dict[\"n_unstable\"][-1])}',\n",
    "          f'lr {results_dict[\"lr\"][-1]}'\n",
    "         )\n",
    "\n",
    "    # anneal the learning rate using just the box metric\n",
    "    #lr_scheduler.step(results_dict[\"val_loss\"][-1])\n",
    "    \n",
    "    if results_dict[\"val_loss\"][-1] == min(results_dict[\"val_loss\"]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': min(results_dict[\"val_loss\"])\n",
    "        }\n",
    "        torch.save(state_dict, f\"gecko/fno.pt\")\n",
    "    \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(results_dict[\"val_loss\"]) if j == min(results_dict[\"val_loss\"])][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# y_pred = model(torch.from_numpy(val_in_array).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
