{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import TensorDataset, Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import copy\n",
    "import time, yaml\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd, numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reliability import reliability_diagram, reliability_diagrams, compute_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    return device\n",
    "\n",
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_evidence(y):\n",
    "    return F.relu(y)\n",
    "\n",
    "\n",
    "def exp_evidence(y):\n",
    "    return torch.exp(torch.clamp(y, -10, 10))\n",
    "\n",
    "\n",
    "def softplus_evidence(y):\n",
    "    return F.softplus(y)\n",
    "\n",
    "\n",
    "def kl_divergence(alpha, num_classes, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n",
    "    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    first_term = (\n",
    "        torch.lgamma(sum_alpha)\n",
    "        - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n",
    "        + torch.lgamma(ones).sum(dim=1, keepdim=True)\n",
    "        - torch.lgamma(ones.sum(dim=1, keepdim=True))\n",
    "    )\n",
    "    second_term = (\n",
    "        (alpha - ones)\n",
    "        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n",
    "        .sum(dim=1, keepdim=True)\n",
    "    )\n",
    "    kl = first_term + second_term\n",
    "    return kl\n",
    "\n",
    "\n",
    "def loglikelihood_loss(y, alpha, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n",
    "    loglikelihood_var = torch.sum(\n",
    "        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True\n",
    "    )\n",
    "    loglikelihood = loglikelihood_err + loglikelihood_var\n",
    "    return loglikelihood\n",
    "\n",
    "\n",
    "def mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    loglikelihood = loglikelihood_loss(y, alpha, device=device)\n",
    "\n",
    "    annealing_coef = torch.min(\n",
    "        torch.tensor(1.0, dtype=torch.float32),\n",
    "        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n",
    "    return loglikelihood + kl_div\n",
    "\n",
    "\n",
    "def edl_loss(func, y, alpha, epoch_num, num_classes, annealing_step, device=None):\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n",
    "\n",
    "    annealing_coef = torch.min(\n",
    "        torch.tensor(1.0, dtype=torch.float32),\n",
    "        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n",
    "    return A + kl_div\n",
    "\n",
    "\n",
    "def edl_mse_loss(output, target, epoch_num, num_classes, annealing_step, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(\n",
    "        mse_loss(target, alpha, epoch_num, num_classes, annealing_step, device=device)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edl_log_loss(output, target, epoch_num, num_classes, annealing_step, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(\n",
    "        edl_loss(\n",
    "            torch.log, target, alpha, epoch_num, num_classes, annealing_step, device\n",
    "        )\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def edl_digamma_loss(\n",
    "    output, target, epoch_num, num_classes, annealing_step, device=None\n",
    "):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(\n",
    "        edl_loss(\n",
    "            torch.digamma, target, alpha, epoch_num, num_classes, annealing_step, device\n",
    "        )\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    num_classes,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=25,\n",
    "    device=None,\n",
    "    uncertainty=False,\n",
    "    metric=\"accuracy\"\n",
    "):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    training_results = defaultdict(list)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                #print(\"Training...\")\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                #print(\"Validating...\")\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            correct = 0\n",
    "            \n",
    "            if verbose:\n",
    "                total = int(np.ceil(len(dataloaders[phase].dataset) / batch_size))\n",
    "                my_iter = tqdm.tqdm(enumerate(dataloaders[phase]),\n",
    "                                total = total,\n",
    "                                leave = True)\n",
    "            else:\n",
    "                my_iter = enumerate(dataloaders[phase])\n",
    "\n",
    "            # Iterate over data.\n",
    "            results_dict = defaultdict(list)\n",
    "            for i, (inputs, labels) in my_iter:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "                    if uncertainty:\n",
    "                        y = one_hot_embedding(labels, num_classes)\n",
    "                        y = y.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(\n",
    "                            outputs, y.float(), epoch, num_classes, 10, device\n",
    "                        )\n",
    "\n",
    "                        match = torch.reshape(torch.eq(preds, labels).float(), (-1, 1))\n",
    "                        acc = torch.mean(match)\n",
    "                        evidence = relu_evidence(outputs)\n",
    "                        alpha = evidence + 1\n",
    "                        u = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "                        total_evidence = torch.sum(evidence, 1, keepdim=True)\n",
    "                        mean_evidence = torch.mean(total_evidence)\n",
    "                        mean_evidence_succ = torch.sum(\n",
    "                            torch.sum(evidence, 1, keepdim=True) * match\n",
    "                        ) / torch.sum(match + 1e-20)\n",
    "                        mean_evidence_fail = torch.sum(\n",
    "                            torch.sum(evidence, 1, keepdim=True) * (1 - match)\n",
    "                        ) / (torch.sum(torch.abs(1 - match)) + 1e-20)\n",
    "\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                results_dict[\"loss\"].append(loss.item())\n",
    "                results_dict[\"acc\"].append(torch.mean((preds == labels.data).float()).item())\n",
    "                \n",
    "                if verbose:\n",
    "                    print_str = f\"Epoch: {epoch} \"\n",
    "                    print_str += f'{phase}_loss: {np.mean(results_dict[\"loss\"]):.4f} '\n",
    "                    print_str += f'{phase}_acc: {np.mean(results_dict[\"acc\"]):.4f}'\n",
    "                    my_iter.set_description(print_str)\n",
    "                    my_iter.refresh()\n",
    "\n",
    "            epoch_loss = np.mean(results_dict[\"loss\"])\n",
    "            epoch_acc = np.mean(results_dict[\"acc\"])\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                training_results[\"train_loss\"].append(epoch_loss)\n",
    "                training_results[\"train_acc\"].append(epoch_acc)\n",
    "            else:\n",
    "                training_results[\"valid_loss\"].append(epoch_loss)\n",
    "                training_results[\"valid_acc\"].append(epoch_acc)\n",
    "            \n",
    "            training_results[\"epoch\"].append(epoch)\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                if phase == \"val\":\n",
    "                    scheduler.step(1-epoch_acc)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            # Stop training if we have not improved after X epochs\n",
    "            if phase == \"val\":\n",
    "                best_epoch = [i for i,j in enumerate(\n",
    "                    training_results[\"valid_acc\"]) if j == max(training_results[\"valid_acc\"])][0]\n",
    "                offset = epoch - best_epoch\n",
    "                if offset >= stopping_patience:\n",
    "                    break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, optimizer, training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"config/p-type.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(conf['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conf['tempvars'] + conf['tempdewvars'] + conf['ugrdvars'] + conf['vgrdvars']\n",
    "outputs = conf['outputvars']\n",
    "num_classes = len(outputs)\n",
    "n_splits = conf['trainer']['n_splits']\n",
    "train_size1 = conf['trainer']['train_size1'] # sets test size\n",
    "train_size2 = conf['trainer']['train_size2'] # sets valid size\n",
    "num_hidden_layers = conf['trainer']['num_hidden_layers']\n",
    "hidden_sizes = conf['trainer']['hidden_sizes']\n",
    "dropout_rate = conf['trainer']['dropout_rate']\n",
    "batch_size = conf['trainer']['batch_size']\n",
    "learning_rate = conf['trainer']['learning_rate']\n",
    "metrics = conf['trainer']['metrics']\n",
    "run_eagerly = conf['trainer']['run_eagerly']\n",
    "shuffle = conf['trainer']['shuffle']\n",
    "epochs = conf['trainer']['epochs']\n",
    "\n",
    "lr_patience = 3\n",
    "stopping_patience = 10\n",
    "\n",
    "verbose = True\n",
    "\n",
    "loss = \"digamma\"\n",
    "use_uncertainty = False if loss == \"ce\" else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split and preprocess the data\n",
    "df['day'] = df['datetime'].apply(lambda x: str(x).split(' ')[0])\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=n_splits, train_size=train_size1)\n",
    "train_idx, test_idx = list(splitter.split(df, groups=df['day']))[0]\n",
    "train_data, test_data = df.iloc[train_idx], df.iloc[test_idx]\n",
    "\n",
    "splitter = GroupShuffleSplit(n_splits=n_splits, train_size=train_size2)\n",
    "train_idx, valid_idx = list(splitter.split(train_data, groups=train_data['day']))[0]\n",
    "train_data, valid_data = train_data.iloc[train_idx], train_data.iloc[valid_idx]\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(train_data[features])\n",
    "x_valid = scaler_x.transform(valid_data[features])\n",
    "x_test = scaler_x.transform(test_data[features])\n",
    "y_train = np.argmax(train_data[outputs].to_numpy(), 1)\n",
    "y_valid = np.argmax(valid_data[outputs].to_numpy(), 1)\n",
    "y_test = np.argmax(test_data[outputs].to_numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = TensorDataset(\n",
    "    torch.from_numpy(x_train).float(),\n",
    "    torch.from_numpy(y_train).long()\n",
    ")\n",
    "train_loader = DataLoader(train_split, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "\n",
    "valid_split = TensorDataset(\n",
    "    torch.from_numpy(x_valid).float(),\n",
    "    torch.from_numpy(y_valid).long()\n",
    ")\n",
    "valid_loader = DataLoader(valid_split, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=0)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": valid_loader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecipNet(nn.Module):\n",
    "    def __init__(self, dropout=False):\n",
    "        super().__init__()\n",
    "        self.use_dropout = dropout\n",
    "        self.conv1 = nn.Conv1d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(20, 50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(20000, 500)\n",
    "        self.fc2 = nn.Linear(500, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 1))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 1))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.use_dropout:\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "def load_mlp_model(input_size, middle_size, output_size, dropout):\n",
    "    model = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Linear(input_size, middle_size)),\n",
    "            #nn.BatchNorm1d(middle_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(),\n",
    "            #nn.Tanh(),\n",
    "            nn.utils.spectral_norm(nn.Linear(middle_size, output_size))\n",
    "    ) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_uncertainty:\n",
    "    if loss == \"digamma\":\n",
    "        criterion = edl_digamma_loss\n",
    "    elif loss == \"log\":\n",
    "        criterion = edl_log_loss\n",
    "    elif loss == \"mse\":\n",
    "        criterion = edl_mse_loss\n",
    "    else:\n",
    "        logging.error(\"--uncertainty requires --mse, --log or --digamma.\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_mlp_model(len(features), 100, len(outputs), dropout_rate) #PrecipNet()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005)\n",
    "\n",
    "#exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = lr_patience, \n",
    "    verbose = verbose,\n",
    "    min_lr = 1.0e-13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train_loss: 0.8490 train_acc: 0.7853: 100%|██████████| 172/172 [00:08<00:00, 20.77it/s]\n",
      "Epoch: 0 val_loss: 0.7608 val_acc: 0.8108: 100%|██████████| 23/23 [00:00<00:00, 28.95it/s]\n",
      "Epoch: 1 train_loss: 0.7918 train_acc: 0.8053: 100%|██████████| 172/172 [00:08<00:00, 21.08it/s]\n",
      "Epoch: 1 val_loss: 0.7819 val_acc: 0.8168: 100%|██████████| 23/23 [00:00<00:00, 28.89it/s]\n",
      "Epoch: 2 train_loss: 0.8018 train_acc: 0.8127: 100%|██████████| 172/172 [00:08<00:00, 20.97it/s]\n",
      "Epoch: 2 val_loss: 0.8112 val_acc: 0.8153: 100%|██████████| 23/23 [00:00<00:00, 28.88it/s]\n",
      "Epoch: 3 train_loss: 0.8216 train_acc: 0.8158: 100%|██████████| 172/172 [00:08<00:00, 20.76it/s]\n",
      "Epoch: 3 val_loss: 0.7983 val_acc: 0.8283: 100%|██████████| 23/23 [00:00<00:00, 31.45it/s]\n",
      "Epoch: 4 train_loss: 0.8400 train_acc: 0.8183: 100%|██████████| 172/172 [00:08<00:00, 20.96it/s]\n",
      "Epoch: 4 val_loss: 0.8354 val_acc: 0.8258: 100%|██████████| 23/23 [00:00<00:00, 28.76it/s]\n",
      "Epoch: 5 train_loss: 0.8595 train_acc: 0.8199: 100%|██████████| 172/172 [00:08<00:00, 20.66it/s]\n",
      "Epoch: 5 val_loss: 0.8476 val_acc: 0.8296: 100%|██████████| 23/23 [00:00<00:00, 28.54it/s]\n",
      "Epoch: 6 train_loss: 0.8749 train_acc: 0.8214: 100%|██████████| 172/172 [00:08<00:00, 20.78it/s]\n",
      "Epoch: 6 val_loss: 0.8713 val_acc: 0.8246: 100%|██████████| 23/23 [00:00<00:00, 28.35it/s]\n",
      "Epoch: 7 train_loss: 0.8885 train_acc: 0.8227: 100%|██████████| 172/172 [00:08<00:00, 20.78it/s]\n",
      "Epoch: 7 val_loss: 0.8804 val_acc: 0.8270: 100%|██████████| 23/23 [00:00<00:00, 28.79it/s]\n",
      "Epoch: 8 train_loss: 0.9033 train_acc: 0.8234: 100%|██████████| 172/172 [00:08<00:00, 20.97it/s]\n",
      "Epoch: 8 val_loss: 0.9157 val_acc: 0.8241: 100%|██████████| 23/23 [00:00<00:00, 28.84it/s]\n",
      "Epoch: 9 train_loss: 0.9161 train_acc: 0.8242: 100%|██████████| 172/172 [00:08<00:00, 20.72it/s]\n",
      "Epoch: 9 val_loss: 0.9061 val_acc: 0.8323: 100%|██████████| 23/23 [00:00<00:00, 28.62it/s]\n",
      "Epoch: 10 train_loss: 0.9259 train_acc: 0.8258: 100%|██████████| 172/172 [00:08<00:00, 20.90it/s]\n",
      "Epoch: 10 val_loss: 0.9156 val_acc: 0.8332: 100%|██████████| 23/23 [00:00<00:00, 28.54it/s]\n",
      "Epoch: 11 train_loss: 0.9265 train_acc: 0.8253: 100%|██████████| 172/172 [00:08<00:00, 20.70it/s]\n",
      "Epoch: 11 val_loss: 0.9317 val_acc: 0.8281: 100%|██████████| 23/23 [00:00<00:00, 28.88it/s]\n",
      "Epoch: 12 train_loss: 0.9270 train_acc: 0.8250: 100%|██████████| 172/172 [00:08<00:00, 20.87it/s]\n",
      "Epoch: 12 val_loss: 0.9199 val_acc: 0.8315: 100%|██████████| 23/23 [00:00<00:00, 28.63it/s]\n",
      "Epoch: 13 train_loss: 0.9263 train_acc: 0.8256: 100%|██████████| 172/172 [00:08<00:00, 20.69it/s]\n",
      "Epoch: 13 val_loss: 0.9200 val_acc: 0.8312: 100%|██████████| 23/23 [00:00<00:00, 28.82it/s]\n",
      "Epoch: 14 train_loss: 0.9262 train_acc: 0.8254: 100%|██████████| 172/172 [00:08<00:00, 20.99it/s]\n",
      "Epoch: 14 val_loss: 0.9345 val_acc: 0.8309: 100%|██████████| 23/23 [00:00<00:00, 28.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15 train_loss: 0.9240 train_acc: 0.8268: 100%|██████████| 172/172 [00:08<00:00, 20.92it/s]\n",
      "Epoch: 15 val_loss: 0.9153 val_acc: 0.8329: 100%|██████████| 23/23 [00:00<00:00, 28.18it/s]\n",
      "Epoch: 16 train_loss: 0.9161 train_acc: 0.8290: 100%|██████████| 172/172 [00:08<00:00, 20.82it/s]\n",
      "Epoch: 16 val_loss: 0.9183 val_acc: 0.8320: 100%|██████████| 23/23 [00:00<00:00, 31.46it/s]\n",
      "Epoch: 17 train_loss: 0.9157 train_acc: 0.8294: 100%|██████████| 172/172 [00:08<00:00, 20.82it/s]\n",
      "Epoch: 17 val_loss: 0.9164 val_acc: 0.8329: 100%|██████████| 23/23 [00:00<00:00, 28.93it/s]\n",
      "Epoch: 18 train_loss: 0.9160 train_acc: 0.8294: 100%|██████████| 172/172 [00:08<00:00, 20.97it/s]\n",
      "Epoch: 18 val_loss: 0.9136 val_acc: 0.8345: 100%|██████████| 23/23 [00:00<00:00, 29.07it/s]\n",
      "Epoch: 19 train_loss: 0.9155 train_acc: 0.8294: 100%|██████████| 172/172 [00:08<00:00, 20.87it/s]\n",
      "Epoch: 19 val_loss: 0.9125 val_acc: 0.8341: 100%|██████████| 23/23 [00:00<00:00, 28.51it/s]\n",
      "Epoch: 20 train_loss: 0.9159 train_acc: 0.8294: 100%|██████████| 172/172 [00:08<00:00, 20.77it/s]\n",
      "Epoch: 20 val_loss: 0.9121 val_acc: 0.8339: 100%|██████████| 23/23 [00:00<00:00, 29.02it/s]\n",
      "Epoch: 21 train_loss: 0.9153 train_acc: 0.8301: 100%|██████████| 172/172 [00:08<00:00, 21.15it/s]\n",
      "Epoch: 21 val_loss: 0.9128 val_acc: 0.8346: 100%|██████████| 23/23 [00:00<00:00, 29.10it/s]\n",
      "Epoch: 22 train_loss: 0.9153 train_acc: 0.8297: 100%|██████████| 172/172 [00:08<00:00, 20.64it/s]\n",
      "Epoch: 22 val_loss: 0.9157 val_acc: 0.8331: 100%|██████████| 23/23 [00:00<00:00, 29.10it/s]\n",
      "Epoch: 23 train_loss: 0.9152 train_acc: 0.8298: 100%|██████████| 172/172 [00:08<00:00, 20.39it/s]\n",
      "Epoch: 23 val_loss: 0.9173 val_acc: 0.8322: 100%|██████████| 23/23 [00:00<00:00, 28.51it/s]\n",
      "Epoch: 24 train_loss: 0.9152 train_acc: 0.8298: 100%|██████████| 172/172 [00:08<00:00, 20.49it/s]\n",
      "Epoch: 24 val_loss: 0.9127 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 29.08it/s]\n",
      "Epoch: 25 train_loss: 0.9149 train_acc: 0.8299: 100%|██████████| 172/172 [00:08<00:00, 20.61it/s]\n",
      "Epoch: 25 val_loss: 0.9123 val_acc: 0.8331: 100%|██████████| 23/23 [00:00<00:00, 28.86it/s]\n",
      "Epoch: 26 train_loss: 0.9146 train_acc: 0.8301: 100%|██████████| 172/172 [00:08<00:00, 20.38it/s]\n",
      "Epoch: 26 val_loss: 0.9116 val_acc: 0.8367: 100%|██████████| 23/23 [00:00<00:00, 28.34it/s]\n",
      "Epoch: 27 train_loss: 0.9149 train_acc: 0.8305: 100%|██████████| 172/172 [00:08<00:00, 20.47it/s]\n",
      "Epoch: 27 val_loss: 0.9137 val_acc: 0.8342: 100%|██████████| 23/23 [00:00<00:00, 30.63it/s]\n",
      "Epoch: 28 train_loss: 0.9150 train_acc: 0.8301: 100%|██████████| 172/172 [00:08<00:00, 20.39it/s]\n",
      "Epoch: 28 val_loss: 0.9146 val_acc: 0.8320: 100%|██████████| 23/23 [00:00<00:00, 28.00it/s]\n",
      "Epoch: 29 train_loss: 0.9150 train_acc: 0.8301: 100%|██████████| 172/172 [00:08<00:00, 20.54it/s]\n",
      "Epoch: 29 val_loss: 0.9181 val_acc: 0.8344: 100%|██████████| 23/23 [00:00<00:00, 30.36it/s]\n",
      "Epoch: 30 train_loss: 0.9145 train_acc: 0.8303: 100%|██████████| 172/172 [00:08<00:00, 20.61it/s]\n",
      "Epoch: 30 val_loss: 0.9123 val_acc: 0.8323: 100%|██████████| 23/23 [00:00<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31 train_loss: 0.9126 train_acc: 0.8310: 100%|██████████| 172/172 [00:08<00:00, 20.36it/s]\n",
      "Epoch: 31 val_loss: 0.9101 val_acc: 0.8344: 100%|██████████| 23/23 [00:00<00:00, 28.75it/s]\n",
      "Epoch: 32 train_loss: 0.9113 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.59it/s]\n",
      "Epoch: 32 val_loss: 0.9113 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.01it/s]\n",
      "Epoch: 33 train_loss: 0.9109 train_acc: 0.8314: 100%|██████████| 172/172 [00:08<00:00, 20.56it/s]\n",
      "Epoch: 33 val_loss: 0.9101 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 27.62it/s]\n",
      "Epoch: 34 train_loss: 0.9111 train_acc: 0.8315: 100%|██████████| 172/172 [00:08<00:00, 20.57it/s]\n",
      "Epoch: 34 val_loss: 0.9106 val_acc: 0.8346: 100%|██████████| 23/23 [00:00<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00035: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 35 train_loss: 0.9110 train_acc: 0.8315: 100%|██████████| 172/172 [00:08<00:00, 20.55it/s]\n",
      "Epoch: 35 val_loss: 0.9112 val_acc: 0.8348: 100%|██████████| 23/23 [00:00<00:00, 29.06it/s]\n",
      "Epoch: 36 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.60it/s]\n",
      "Epoch: 36 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.06it/s]\n",
      "Epoch: 37 train_loss: 0.9105 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.42it/s]\n",
      "Epoch: 37 val_loss: 0.9105 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 28.54it/s]\n",
      "Epoch: 38 train_loss: 0.9104 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.57it/s]\n",
      "Epoch: 38 val_loss: 0.9113 val_acc: 0.8349: 100%|██████████| 23/23 [00:00<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00039: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 39 train_loss: 0.9108 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.46it/s]\n",
      "Epoch: 39 val_loss: 0.9110 val_acc: 0.8349: 100%|██████████| 23/23 [00:00<00:00, 27.77it/s]\n",
      "Epoch: 40 train_loss: 0.9104 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.36it/s]\n",
      "Epoch: 40 val_loss: 0.9108 val_acc: 0.8349: 100%|██████████| 23/23 [00:00<00:00, 31.24it/s]\n",
      "Epoch: 41 train_loss: 0.9104 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.47it/s]\n",
      "Epoch: 41 val_loss: 0.9107 val_acc: 0.8349: 100%|██████████| 23/23 [00:00<00:00, 27.83it/s]\n",
      "Epoch: 42 train_loss: 0.9105 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.53it/s]\n",
      "Epoch: 42 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00043: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 43 train_loss: 0.9109 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.58it/s]\n",
      "Epoch: 43 val_loss: 0.9111 val_acc: 0.8349: 100%|██████████| 23/23 [00:00<00:00, 27.82it/s]\n",
      "Epoch: 44 train_loss: 0.9110 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.47it/s]\n",
      "Epoch: 44 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.60it/s]\n",
      "Epoch: 45 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.64it/s]\n",
      "Epoch: 45 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.86it/s]\n",
      "Epoch: 46 train_loss: 0.9107 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.52it/s]\n",
      "Epoch: 46 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.96it/s]\n",
      "Epoch: 47 train_loss: 0.9108 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.70it/s]\n",
      "Epoch: 47 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.95it/s]\n",
      "Epoch: 48 train_loss: 0.9108 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.59it/s]\n",
      "Epoch: 48 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.56it/s]\n",
      "Epoch: 49 train_loss: 0.9107 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.68it/s]\n",
      "Epoch: 49 val_loss: 0.9111 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.59it/s]\n",
      "Epoch: 50 train_loss: 0.9108 train_acc: 0.8315: 100%|██████████| 172/172 [00:08<00:00, 20.52it/s]\n",
      "Epoch: 50 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.59it/s]\n",
      "Epoch: 51 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.48it/s]\n",
      "Epoch: 51 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 30.96it/s]\n",
      "Epoch: 52 train_loss: 0.9107 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.50it/s]\n",
      "Epoch: 52 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.80it/s]\n",
      "Epoch: 53 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.42it/s]\n",
      "Epoch: 53 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 30.83it/s]\n",
      "Epoch: 54 train_loss: 0.9108 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.39it/s]\n",
      "Epoch: 54 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.57it/s]\n",
      "Epoch: 55 train_loss: 0.9108 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.45it/s]\n",
      "Epoch: 55 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.90it/s]\n",
      "Epoch: 56 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.64it/s]\n",
      "Epoch: 56 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.57it/s]\n",
      "Epoch: 57 train_loss: 0.9105 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.48it/s]\n",
      "Epoch: 57 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.72it/s]\n",
      "Epoch: 58 train_loss: 0.9107 train_acc: 0.8321: 100%|██████████| 172/172 [00:08<00:00, 20.60it/s]\n",
      "Epoch: 58 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.14it/s]\n",
      "Epoch: 59 train_loss: 0.9106 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.55it/s]\n",
      "Epoch: 59 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.95it/s]\n",
      "Epoch: 60 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.81it/s]\n",
      "Epoch: 60 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.89it/s]\n",
      "Epoch: 61 train_loss: 0.9108 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.53it/s]\n",
      "Epoch: 61 val_loss: 0.9110 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 27.78it/s]\n",
      "Epoch: 62 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.47it/s]\n",
      "Epoch: 62 val_loss: 0.9109 val_acc: 0.8350: 100%|██████████| 23/23 [00:00<00:00, 28.90it/s]\n",
      "Epoch: 63 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.44it/s]\n",
      "Epoch: 63 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.38it/s]\n",
      "Epoch: 64 train_loss: 0.9107 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.58it/s]\n",
      "Epoch: 64 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 30.51it/s]\n",
      "Epoch: 65 train_loss: 0.9106 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.49it/s]\n",
      "Epoch: 65 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.10it/s]\n",
      "Epoch: 66 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.55it/s]\n",
      "Epoch: 66 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.61it/s]\n",
      "Epoch: 67 train_loss: 0.9105 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.67it/s]\n",
      "Epoch: 67 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 27.79it/s]\n",
      "Epoch: 68 train_loss: 0.9107 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.42it/s]\n",
      "Epoch: 68 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 26.21it/s]\n",
      "Epoch: 69 train_loss: 0.9106 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.58it/s]\n",
      "Epoch: 69 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.40it/s]\n",
      "Epoch: 70 train_loss: 0.9107 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.38it/s]\n",
      "Epoch: 70 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.17it/s]\n",
      "Epoch: 71 train_loss: 0.9106 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.75it/s]\n",
      "Epoch: 71 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 27.93it/s]\n",
      "Epoch: 72 train_loss: 0.9104 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.56it/s]\n",
      "Epoch: 72 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.36it/s]\n",
      "Epoch: 73 train_loss: 0.9107 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.73it/s]\n",
      "Epoch: 73 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 29.30it/s]\n",
      "Epoch: 74 train_loss: 0.9106 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.44it/s]\n",
      "Epoch: 74 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 27.80it/s]\n",
      "Epoch: 75 train_loss: 0.9106 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.55it/s]\n",
      "Epoch: 75 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 30.62it/s]\n",
      "Epoch: 76 train_loss: 0.9105 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.52it/s]\n",
      "Epoch: 76 val_loss: 0.9109 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 29.08it/s]\n",
      "Epoch: 77 train_loss: 0.9105 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.47it/s]\n",
      "Epoch: 77 val_loss: 0.9108 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 30.11it/s]\n",
      "Epoch: 78 train_loss: 0.9108 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.37it/s]\n",
      "Epoch: 78 val_loss: 0.9108 val_acc: 0.8351: 100%|██████████| 23/23 [00:00<00:00, 28.12it/s]\n",
      "Epoch: 79 train_loss: 0.9104 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.53it/s]\n",
      "Epoch: 79 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.96it/s]\n",
      "Epoch: 80 train_loss: 0.9104 train_acc: 0.8321: 100%|██████████| 172/172 [00:08<00:00, 20.76it/s]\n",
      "Epoch: 80 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 27.19it/s]\n",
      "Epoch: 81 train_loss: 0.9105 train_acc: 0.8317: 100%|██████████| 172/172 [00:08<00:00, 20.58it/s]\n",
      "Epoch: 81 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.25it/s]\n",
      "Epoch: 82 train_loss: 0.9106 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.71it/s]\n",
      "Epoch: 82 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 29.14it/s]\n",
      "Epoch: 83 train_loss: 0.9106 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.56it/s]\n",
      "Epoch: 83 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 27.70it/s]\n",
      "Epoch: 84 train_loss: 0.9105 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.67it/s]\n",
      "Epoch: 84 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.62it/s]\n",
      "Epoch: 85 train_loss: 0.9107 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.59it/s]\n",
      "Epoch: 85 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.14it/s]\n",
      "Epoch: 86 train_loss: 0.9105 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.70it/s]\n",
      "Epoch: 86 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 27.80it/s]\n",
      "Epoch: 87 train_loss: 0.9105 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.30it/s]\n",
      "Epoch: 87 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 28.66it/s]\n",
      "Epoch: 88 train_loss: 0.9106 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.39it/s]\n",
      "Epoch: 88 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 30.01it/s]\n",
      "Epoch: 89 train_loss: 0.9106 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.55it/s]\n",
      "Epoch: 89 val_loss: 0.9108 val_acc: 0.8352: 100%|██████████| 23/23 [00:00<00:00, 29.20it/s]\n",
      "Epoch: 90 train_loss: 0.9104 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.42it/s]\n",
      "Epoch: 90 val_loss: 0.9108 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 27.75it/s]\n",
      "Epoch: 91 train_loss: 0.9106 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.70it/s]\n",
      "Epoch: 91 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 28.61it/s]\n",
      "Epoch: 92 train_loss: 0.9105 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.56it/s]\n",
      "Epoch: 92 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 28.33it/s]\n",
      "Epoch: 93 train_loss: 0.9102 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.71it/s]\n",
      "Epoch: 93 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 27.90it/s]\n",
      "Epoch: 94 train_loss: 0.9104 train_acc: 0.8323: 100%|██████████| 172/172 [00:08<00:00, 20.58it/s]\n",
      "Epoch: 94 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 29.50it/s]\n",
      "Epoch: 95 train_loss: 0.9105 train_acc: 0.8319: 100%|██████████| 172/172 [00:08<00:00, 20.78it/s]\n",
      "Epoch: 95 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 27.74it/s]\n",
      "Epoch: 96 train_loss: 0.9105 train_acc: 0.8318: 100%|██████████| 172/172 [00:08<00:00, 20.46it/s]\n",
      "Epoch: 96 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 28.87it/s]\n",
      "Epoch: 97 train_loss: 0.9105 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.67it/s]\n",
      "Epoch: 97 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 28.74it/s]\n",
      "Epoch: 98 train_loss: 0.9103 train_acc: 0.8320: 100%|██████████| 172/172 [00:08<00:00, 20.46it/s]\n",
      "Epoch: 98 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 27.89it/s]\n",
      "Epoch: 99 train_loss: 0.9106 train_acc: 0.8316: 100%|██████████| 172/172 [00:08<00:00, 20.44it/s]\n",
      "Epoch: 99 val_loss: 0.9107 val_acc: 0.8353: 100%|██████████| 23/23 [00:00<00:00, 31.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 15m 15s\n",
      "Best val Acc: 0.836696\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, metrics = train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    num_classes,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=lr_scheduler,\n",
    "    num_epochs=100,\n",
    "    device=device,\n",
    "    uncertainty=use_uncertainty,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = TensorDataset(\n",
    "    torch.from_numpy(x_test).float(),\n",
    "    torch.from_numpy(y_test).long()\n",
    ")\n",
    "test_loader = DataLoader(test_split, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    if verbose:\n",
    "        total = int(np.ceil(len(test_loader.dataset) / batch_size))\n",
    "        my_iter = tqdm.tqdm(enumerate(test_loader),\n",
    "                        total = total,\n",
    "                        leave = True)\n",
    "    else:\n",
    "        my_iter = enumerate(test_loader)\n",
    "\n",
    "    # Iterate over data.\n",
    "    results_dict = defaultdict(list)\n",
    "    for i, (inputs, labels) in my_iter:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        if use_uncertainty:\n",
    "            output = model(inputs)\n",
    "            evidence = relu_evidence(output)\n",
    "            alpha = evidence + 1\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            results_dict[\"pred_uncertainty\"].append(uncertainty)\n",
    "\n",
    "        else:\n",
    "            output = model(img_variable)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "        results_dict[\"pred_labels\"].append(preds.unsqueeze(-1))\n",
    "        results_dict[\"true_labels\"].append(labels.unsqueeze(-1))\n",
    "        results_dict[\"pred_probs\"].append(prob)\n",
    "\n",
    "        # statistics\n",
    "        results_dict[\"acc\"].append(torch.mean((preds == labels.data).float()).item())\n",
    "\n",
    "        if verbose:\n",
    "            print_str = f'test_acc: {np.mean(results_dict[\"acc\"]):.4f}'\n",
    "            my_iter.set_description(print_str)\n",
    "            my_iter.refresh()\n",
    "\n",
    "    results_dict[\"pred_uncertainty\"] = torch.cat(results_dict[\"pred_uncertainty\"], 0)\n",
    "    results_dict[\"pred_probs\"] = torch.cat(results_dict[\"pred_probs\"], 0)\n",
    "    results_dict[\"pred_labels\"] = torch.cat(results_dict[\"pred_labels\"], 0)\n",
    "    results_dict[\"true_labels\"] = torch.cat(results_dict[\"true_labels\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(outputs)):\n",
    "    test_data[f\"{outputs[idx]}_conf\"] = results_dict[\"pred_probs\"][:, idx].cpu().numpy()\n",
    "test_data[\"uncertainty\"] = results_dict[\"pred_uncertainty\"][:, 0].cpu().numpy()\n",
    "test_data[\"pred_labels\"] = results_dict[\"pred_labels\"][:, 0].cpu().numpy()\n",
    "test_data[\"true_labels\"] = results_dict[\"true_labels\"][:, 0].cpu().numpy()\n",
    "test_data[\"pred_conf\"] = np.max(results_dict[\"pred_probs\"].cpu().numpy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (test_data[\"true_labels\"] == 0)\n",
    "\n",
    "title = \"p-type\"\n",
    "fig = reliability_diagram(\n",
    "    test_data[cond][\"true_labels\"].to_numpy(), \n",
    "    test_data[cond][\"pred_labels\"].to_numpy(), \n",
    "    test_data[cond][\"pred_conf\"].to_numpy(), \n",
    "    num_bins=10, draw_ece=True,\n",
    "    draw_bin_importance=\"alpha\", draw_averages=True,\n",
    "    title=title, figsize=(5, 5), dpi=100, \n",
    "    return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond0 = (test_data[\"true_labels\"] == 0)\n",
    "cond1 = (test_data[\"true_labels\"] == 1)\n",
    "cond2 = (test_data[\"true_labels\"] == 2)\n",
    "cond3 = (test_data[\"true_labels\"] == 3)\n",
    "\n",
    "results = OrderedDict()\n",
    "results[outputs[0]] = {\n",
    "    \"true_labels\": test_data[cond0][\"true_labels\"].values, \n",
    "    \"pred_labels\": test_data[cond0][\"pred_labels\"].values, \n",
    "    \"confidences\": test_data[cond0][\"pred_conf\"].values\n",
    "}\n",
    "results[outputs[1]] = {\n",
    "    \"true_labels\": test_data[cond1][\"true_labels\"].values, \n",
    "    \"pred_labels\": test_data[cond1][\"pred_labels\"].values, \n",
    "    \"confidences\": test_data[cond1][\"pred_conf\"].values\n",
    "}\n",
    "results[outputs[2]] = {\n",
    "    \"true_labels\": test_data[cond2][\"true_labels\"].values, \n",
    "    \"pred_labels\": test_data[cond2][\"pred_labels\"].values, \n",
    "    \"confidences\": test_data[cond2][\"pred_conf\"].values\n",
    "}\n",
    "results[outputs[3]] = {\n",
    "    \"true_labels\": test_data[cond3][\"true_labels\"].values, \n",
    "    \"pred_labels\": test_data[cond3][\"pred_labels\"].values, \n",
    "    \"confidences\": test_data[cond3][\"pred_conf\"].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = reliability_diagrams(results, num_bins=10, draw_bin_importance=\"alpha\",\n",
    "                           num_cols=2, dpi=100, return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov(df, col = \"pred_conf\", quan = \"uncertainty\", ascending = False):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(col, ascending = ascending)\n",
    "    df[\"dummy\"] = 1\n",
    "    df[f\"cu_{quan}\"] = df[quan].cumsum() / df[\"dummy\"].cumsum()\n",
    "    df[f\"cu_{col}\"] = df[col].cumsum() / df[\"dummy\"].cumsum()\n",
    "    df[f\"{col}_cov\"] = df[\"dummy\"].cumsum() / len(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sorted = compute_cov(test_data, col = \"pred_conf\", quan = \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(\n",
    "    test_data_sorted[\"pred_conf_cov\"],\n",
    "    test_data_sorted[\"cu_acc\"]\n",
    ")\n",
    "ax.set_ylabel(\"Cumulative accuracy\")\n",
    "ax.set_xlabel(\"Coverage (sorted by predicted confidence)\")\n",
    "#test_data_sorted[outputs + [f\"{x}_conf\" for x in outputs] + [\"acc\", \"uncertainty\", \"pred_conf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "test_data_sorted = compute_cov(test_data, col = \"uncertainty\", quan = \"acc\", ascending = True)\n",
    "\n",
    "l1, = ax.plot(\n",
    "    test_data_sorted[\"uncertainty\"],\n",
    "    test_data_sorted[\"cu_acc\"],\n",
    "    label = \"sigma\"\n",
    ")\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "l2, = ax2.plot(\n",
    "    test_data_sorted[\"uncertainty_cov\"],\n",
    "    test_data_sorted[\"cu_acc\"], \n",
    "    color='orange', ls = \"--\", \n",
    "    label = \"fraction\")\n",
    "\n",
    "ax.set_ylabel(\"Cumulative accuracy\")\n",
    "ax2.set_xlabel(\"Test data fraction\")\n",
    "ax.set_xlabel(\"Predicted uncertainty\")\n",
    "\n",
    "plt.legend([l1, l2], [\"sigma\", \"fraction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "test_data_sorted = compute_cov(test_data, col = \"pred_conf\", quan = \"uncertainty\")\n",
    "\n",
    "ax.plot(\n",
    "    test_data_sorted[\"pred_conf_cov\"],\n",
    "    test_data_sorted[\"cu_uncertainty\"]\n",
    ")\n",
    "ax.set_ylabel(\"Cumulative uncertainty\")\n",
    "ax.set_xlabel(\"Coverage (sorted by predicted confidence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "test_data_sorted = compute_cov(test_data, col = \"pred_conf\", quan = \"uncertainty\")\n",
    "\n",
    "ax.plot(\n",
    "    test_data_sorted[\"pred_conf\"],\n",
    "    test_data_sorted[\"uncertainty\"]\n",
    ")\n",
    "ax.set_ylabel(\"Uncertainty\")\n",
    "ax.set_xlabel(\"Prediction confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
