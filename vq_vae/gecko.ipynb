{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import TensorDataset, Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "\n",
    "import tqdm\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from geckoml.data import load_data, transform_data, inv_transform_preds\n",
    "from geckoml.metrics import ensembled_metrics\n",
    "from collections import defaultdict\n",
    "\n",
    "from geckoml.box import rnn_box_test\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/glade/work/schreck/repos/GECKO_OPT/dev/gecko-ml/config/toluene_agg.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = conf['species']\n",
    "data_path = conf['dir_path']\n",
    "aggregate_bins = conf['aggregate_bins']\n",
    "input_vars = conf['input_vars']\n",
    "output_vars = conf['output_vars']\n",
    "tendency_cols = conf['tendency_cols']\n",
    "log_trans_cols = conf['log_trans_cols']\n",
    "output_path = \"./\"\n",
    "scaler_type = conf['scaler_type']\n",
    "ensemble_members = conf[\"ensemble_members\"]\n",
    "seed = conf['random_seed']\n",
    "\n",
    "# Get the shapes of the input and output data \n",
    "input_size = len(input_vars)\n",
    "output_size = len(output_vars)\n",
    "\n",
    "start_time = 0\n",
    "num_timesteps = 1439\n",
    "batch_size = 256\n",
    "\n",
    "L1_penalty = 1.39e-5\n",
    "L2_penalty = 3.49e-4\n",
    "\n",
    "lr_patience = 3\n",
    "stopping_patience = 5\n",
    "learning_rate = 1.39e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Precursor [ug/m3]', 'Gas [ug/m3]', 'Aerosol [ug_m3]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(data_path, aggregate_bins, species, input_vars, output_vars, log_trans_cols)\n",
    "    \n",
    "transformed_data, x_scaler, y_scaler = transform_data(\n",
    "    data, \n",
    "    output_path, \n",
    "    species, \n",
    "    tendency_cols, \n",
    "    log_trans_cols,\n",
    "    scaler_type, \n",
    "    output_vars, \n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Batch the training data by experiment\n",
    "train_in_array = transformed_data['train_in'].copy()\n",
    "n_exps = len(train_in_array.index.unique(level='id'))\n",
    "n_timesteps = len(train_in_array.index.unique(level='Time [s]'))\n",
    "n_features = len(input_vars)\n",
    "out_col_idx = train_in_array.columns.get_indexer(output_vars)\n",
    "train_in_array = train_in_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "# Batch the validation data by experiment\n",
    "val_in_array = transformed_data['val_in'].copy()\n",
    "n_exps = len(val_in_array.index.unique(level='id'))\n",
    "n_timesteps = len(val_in_array.index.unique(level='Time [s]'))\n",
    "val_out_col_idx = val_in_array.columns.get_indexer(output_vars)\n",
    "val_in_array = val_in_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "train_out_array = transformed_data['train_out'].copy()\n",
    "n_exps = len(train_out_array.index.unique(level='id'))\n",
    "n_timesteps = len(train_out_array.index.unique(level='Time [s]'))\n",
    "n_features = len(output_vars)\n",
    "out_col_idx = train_out_array.columns.get_indexer(output_vars)\n",
    "train_out_array = train_out_array.values.reshape(n_exps, n_timesteps, n_features)\n",
    "\n",
    "val_out_array = transformed_data['val_out'].copy()\n",
    "n_exps = len(val_out_array.index.unique(level='id'))\n",
    "n_timesteps = len(val_out_array.index.unique(level='Time [s]'))\n",
    "val_out_col_idx = val_out_array.columns.get_indexer(output_vars)\n",
    "val_out_array = val_out_array.values.reshape(n_exps, n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    torch.from_numpy(transformed_data[\"train_in\"].copy().values).float(),\n",
    "    torch.from_numpy(transformed_data[\"train_out\"].copy().values).float()\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "valid_data = TensorDataset(\n",
    "    torch.from_numpy(transformed_data[\"val_in\"].copy().values).float(),\n",
    "    torch.from_numpy(transformed_data[\"val_out\"].copy().values).float()\n",
    ")\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if type(m) in [nn.Linear]:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        #nn.init.xavier_uniform_(model.bias)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d or nn.BatchNorm1d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 9\n",
    "middle_size = 4902\n",
    "output_size = 3\n",
    "dr = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(input_size, middle_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(middle_size, output_size)\n",
    ")\n",
    "        \n",
    "model = model.apply(initialize_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63729"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model = load_model(\n",
    "#     '/glade/work/schreck/repos/GECKO_OPT/dev/gecko-ml/results/mlp/no_prec/toluene/models/toluene_DNN_0/',\n",
    "#     compile = False\n",
    "# )\n",
    "# keras_model.compile(metrics = [\"mae\"], loss = [\"mae\"])\n",
    "# keras_model.summary()\n",
    "# weights = keras_model.get_weights()\n",
    "\n",
    "# for layer in keras_model.layers:\n",
    "#     print(layer.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model[0].weight.data=torch.from_numpy(np.transpose(weights[0]))\n",
    "# model[0].bias.data=torch.from_numpy(weights[1])\n",
    "# model[1].weight.data=torch.from_numpy(np.transpose(weights[2]))\n",
    "# model[1].bias.data=torch.from_numpy(weights[3])\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = learning_rate,\n",
    "                             eps = 1e-7,\n",
    "                             betas = (0.9, 0.999),\n",
    "                             amsgrad = False)#,\n",
    "                             #weight_decay = L2_penalty)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer, \n",
    "#     T_max=10, \n",
    "#     eta_min=1e-3*learning_rate\n",
    "# )\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, \n",
    "#         patience = lr_patience, \n",
    "#         verbose = True,\n",
    "#         min_lr = 1.0e-13\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.583948 val_loss 0.378858 step_loss 0.626657 box_mae 0.003039 n_unstable 0 lr 1.39e-05\n",
      "Epoch 1 train_loss 0.262337 val_loss 0.134695 step_loss 0.566282 box_mae 0.002775 n_unstable 0 lr 1.39e-05\n",
      "Epoch 2 train_loss 0.096616 val_loss 0.035953 step_loss 0.664230 box_mae 0.003119 n_unstable 0 lr 1.39e-05\n",
      "Epoch 3 train_loss 0.049356 val_loss 0.016455 step_loss 0.725331 box_mae 0.005137 n_unstable 3 lr 1.39e-05\n",
      "Epoch 4 train_loss 0.038472 val_loss 0.011553 step_loss 0.705827 box_mae 0.005379 n_unstable 4 lr 1.39e-05\n",
      "Epoch 5 train_loss 0.034111 val_loss 0.008759 step_loss 0.694561 box_mae 0.004965 n_unstable 0 lr 1.39e-05\n",
      "Epoch 6 train_loss 0.031321 val_loss 0.006981 step_loss 0.660722 box_mae 0.004523 n_unstable 0 lr 1.39e-05\n",
      "Epoch 7 train_loss 0.029332 val_loss 0.005727 step_loss 0.652781 box_mae 0.004603 n_unstable 0 lr 1.39e-05\n",
      "Epoch 8 train_loss 0.027844 val_loss 0.004874 step_loss 0.640886 box_mae 0.004593 n_unstable 0 lr 1.39e-05\n",
      "Epoch 9 train_loss 0.026693 val_loss 0.004248 step_loss 0.632055 box_mae 0.006438 n_unstable 0 lr 1.39e-05\n",
      "Epoch 10 train_loss 0.025768 val_loss 0.003814 step_loss 0.616869 box_mae 0.007148 n_unstable 0 lr 1.39e-05\n",
      "Epoch 11 train_loss 0.024995 val_loss 0.003516 step_loss 0.622933 box_mae 0.007788 n_unstable 3 lr 1.39e-05\n",
      "Epoch 12 train_loss 0.024328 val_loss 0.003263 step_loss 0.573367 box_mae 0.005666 n_unstable 3 lr 1.39e-05\n",
      "Epoch 13 train_loss 0.023735 val_loss 0.003092 step_loss 0.587015 box_mae 0.007126 n_unstable 4 lr 1.39e-05\n",
      "Epoch 14 train_loss 0.023197 val_loss 0.002920 step_loss 0.573017 box_mae 0.005520 n_unstable 5 lr 1.39e-05\n",
      "Epoch 15 train_loss 0.022703 val_loss 0.002807 step_loss 0.556794 box_mae 0.005219 n_unstable 5 lr 1.39e-05\n",
      "Epoch 16 train_loss 0.022248 val_loss 0.002695 step_loss 0.560510 box_mae 0.005544 n_unstable 7 lr 1.39e-05\n",
      "Epoch 17 train_loss 0.021828 val_loss 0.002597 step_loss 0.566067 box_mae 0.004811 n_unstable 7 lr 1.39e-05\n",
      "Epoch 18 train_loss 0.021433 val_loss 0.002506 step_loss 0.566894 box_mae 0.006197 n_unstable 10 lr 1.39e-05\n",
      "Epoch 19 train_loss 0.021061 val_loss 0.002444 step_loss 0.563409 box_mae 0.004801 n_unstable 14 lr 1.39e-05\n",
      "Epoch 20 train_loss 0.020707 val_loss 0.002383 step_loss 0.542353 box_mae 0.005382 n_unstable 7 lr 1.39e-05\n",
      "Epoch 21 train_loss 0.020368 val_loss 0.002319 step_loss 0.501167 box_mae 0.005468 n_unstable 5 lr 1.39e-05\n",
      "Epoch 22 train_loss 0.020046 val_loss 0.002268 step_loss 0.546307 box_mae 0.006715 n_unstable 13 lr 1.39e-05\n",
      "Epoch 23 train_loss 0.019738 val_loss 0.002220 step_loss 0.527497 box_mae 0.005147 n_unstable 10 lr 1.39e-05\n",
      "Epoch 24 train_loss 0.019445 val_loss 0.002170 step_loss 0.535410 box_mae 0.006941 n_unstable 9 lr 1.39e-05\n",
      "Epoch 25 train_loss 0.019166 val_loss 0.002134 step_loss 0.551283 box_mae 0.006384 n_unstable 14 lr 1.39e-05\n",
      "Epoch 26 train_loss 0.018898 val_loss 0.002098 step_loss 0.484978 box_mae 0.005444 n_unstable 6 lr 1.39e-05\n",
      "Epoch 27 train_loss 0.018643 val_loss 0.002065 step_loss 0.587471 box_mae 0.005169 n_unstable 14 lr 1.39e-05\n",
      "Epoch 28 train_loss 0.018398 val_loss 0.002032 step_loss 0.629608 box_mae 0.005014 n_unstable 19 lr 1.39e-05\n",
      "Epoch 29 train_loss 0.018163 val_loss 0.002011 step_loss 0.530275 box_mae 0.004032 n_unstable 12 lr 1.39e-05\n",
      "Epoch 30 train_loss 0.017936 val_loss 0.001965 step_loss 0.534254 box_mae 0.003847 n_unstable 12 lr 1.39e-05\n",
      "Epoch 31 train_loss 0.017718 val_loss 0.001933 step_loss 0.552216 box_mae 0.003201 n_unstable 11 lr 1.39e-05\n",
      "Epoch 32 train_loss 0.017510 val_loss 0.001904 step_loss 0.582833 box_mae 0.003166 n_unstable 12 lr 1.39e-05\n",
      "Epoch 33 train_loss 0.017309 val_loss 0.001882 step_loss 0.714647 box_mae 0.003624 n_unstable 16 lr 1.39e-05\n",
      "Epoch 34 train_loss 0.017117 val_loss 0.001851 step_loss 0.674859 box_mae 0.003269 n_unstable 13 lr 1.39e-05\n",
      "Epoch 35 train_loss 0.016933 val_loss 0.001845 step_loss 0.708158 box_mae 0.003033 n_unstable 13 lr 1.39e-05\n",
      "Epoch 36 train_loss 0.016752 val_loss 0.001807 step_loss 0.777411 box_mae 0.002896 n_unstable 15 lr 1.39e-05\n",
      "Epoch 37 train_loss 0.016577 val_loss 0.001785 step_loss 0.724675 box_mae 0.002816 n_unstable 13 lr 1.39e-05\n",
      "Epoch 38 train_loss 0.016407 val_loss 0.001769 step_loss 0.783738 box_mae 0.003177 n_unstable 13 lr 1.39e-05\n",
      "Epoch 39 train_loss 0.016244 val_loss 0.001751 step_loss 0.837077 box_mae 0.002980 n_unstable 13 lr 1.39e-05\n",
      "Epoch 40 train_loss 0.016085 val_loss 0.001717 step_loss 1.063220 box_mae 0.003162 n_unstable 17 lr 1.39e-05\n",
      "Epoch 41 train_loss 0.015931 val_loss 0.001697 step_loss 0.982868 box_mae 0.002548 n_unstable 14 lr 1.39e-05\n",
      "Epoch 42 train_loss 0.015781 val_loss 0.001669 step_loss 1.081259 box_mae 0.002915 n_unstable 17 lr 1.39e-05\n",
      "Epoch 43 train_loss 0.015636 val_loss 0.001658 step_loss 1.218384 box_mae 0.002160 n_unstable 16 lr 1.39e-05\n",
      "Epoch 44 train_loss 0.015494 val_loss 0.001640 step_loss 1.152534 box_mae 0.002257 n_unstable 13 lr 1.39e-05\n",
      "Epoch 45 train_loss 0.015358 val_loss 0.001610 step_loss 1.373905 box_mae 0.002280 n_unstable 17 lr 1.39e-05\n",
      "Epoch 46 train_loss 0.015226 val_loss 0.001611 step_loss 1.209474 box_mae 0.001865 n_unstable 15 lr 1.39e-05\n",
      "Epoch 47 train_loss 0.015097 val_loss 0.001584 step_loss 0.949708 box_mae 0.001955 n_unstable 10 lr 1.39e-05\n",
      "Epoch 48 train_loss 0.014973 val_loss 0.001559 step_loss 1.273455 box_mae 0.002312 n_unstable 14 lr 1.39e-05\n",
      "Epoch 49 train_loss 0.014852 val_loss 0.001539 step_loss 1.540211 box_mae 0.001962 n_unstable 16 lr 1.39e-05\n",
      "Epoch 50 train_loss 0.014734 val_loss 0.001524 step_loss 2.647041 box_mae 0.002665 n_unstable 25 lr 1.39e-05\n",
      "Epoch 51 train_loss 0.014619 val_loss 0.001519 step_loss 1.498632 box_mae 0.002216 n_unstable 14 lr 1.39e-05\n",
      "Epoch 52 train_loss 0.014508 val_loss 0.001491 step_loss 2.497639 box_mae 0.002079 n_unstable 19 lr 1.39e-05\n",
      "Epoch 53 train_loss 0.014401 val_loss 0.001465 step_loss 1.970490 box_mae 0.002072 n_unstable 17 lr 1.39e-05\n",
      "Epoch 54 train_loss 0.014296 val_loss 0.001444 step_loss 2.466113 box_mae 0.001958 n_unstable 17 lr 1.39e-05\n",
      "Epoch 55 train_loss 0.014194 val_loss 0.001438 step_loss 2.799358 box_mae 0.001981 n_unstable 23 lr 1.39e-05\n",
      "Epoch 56 train_loss 0.014094 val_loss 0.001433 step_loss 0.632263 box_mae 0.001737 n_unstable 5 lr 1.39e-05\n",
      "Epoch 57 train_loss 0.013998 val_loss 0.001402 step_loss 1.745911 box_mae 0.001942 n_unstable 12 lr 1.39e-05\n",
      "Epoch 58 train_loss 0.013904 val_loss 0.001396 step_loss 1.008396 box_mae 0.001841 n_unstable 9 lr 1.39e-05\n",
      "Epoch 59 train_loss 0.013814 val_loss 0.001384 step_loss 0.660612 box_mae 0.001644 n_unstable 6 lr 1.39e-05\n",
      "Epoch 60 train_loss 0.013725 val_loss 0.001369 step_loss 0.782062 box_mae 0.001721 n_unstable 7 lr 1.39e-05\n",
      "Epoch 61 train_loss 0.013639 val_loss 0.001349 step_loss 0.825641 box_mae 0.001995 n_unstable 4 lr 1.39e-05\n",
      "Epoch 62 train_loss 0.013557 val_loss 0.001336 step_loss 1.237183 box_mae 0.001774 n_unstable 12 lr 1.39e-05\n",
      "Epoch 63 train_loss 0.013477 val_loss 0.001328 step_loss 1.535397 box_mae 0.001938 n_unstable 14 lr 1.39e-05\n",
      "Epoch 64 train_loss 0.013399 val_loss 0.001332 step_loss 3.268701 box_mae 0.002317 n_unstable 27 lr 1.39e-05\n",
      "Epoch 65 train_loss 0.013323 val_loss 0.001311 step_loss 0.879434 box_mae 0.001573 n_unstable 5 lr 1.39e-05\n",
      "Epoch 66 train_loss 0.013249 val_loss 0.001330 step_loss 2.374869 box_mae 0.001743 n_unstable 27 lr 1.39e-05\n",
      "Epoch 67 train_loss 0.013178 val_loss 0.001290 step_loss 0.494212 box_mae 0.001538 n_unstable 2 lr 1.39e-05\n",
      "Epoch 68 train_loss 0.013106 val_loss 0.001291 step_loss 0.435523 box_mae 0.001308 n_unstable 3 lr 1.39e-05\n",
      "Epoch 69 train_loss 0.013039 val_loss 0.001258 step_loss 0.589114 box_mae 0.001747 n_unstable 3 lr 1.39e-05\n",
      "Epoch 70 train_loss 0.012971 val_loss 0.001248 step_loss 1.041418 box_mae 0.001927 n_unstable 13 lr 1.39e-05\n",
      "Epoch 71 train_loss 0.012906 val_loss 0.001243 step_loss 1.319659 box_mae 0.001597 n_unstable 11 lr 1.39e-05\n",
      "Epoch 72 train_loss 0.012843 val_loss 0.001248 step_loss 0.552043 box_mae 0.001517 n_unstable 4 lr 1.39e-05\n",
      "Epoch 73 train_loss 0.012782 val_loss 0.001229 step_loss 1.008204 box_mae 0.001520 n_unstable 10 lr 1.39e-05\n",
      "Epoch 74 train_loss 0.012722 val_loss 0.001237 step_loss 0.787310 box_mae 0.001264 n_unstable 4 lr 1.39e-05\n",
      "Epoch 75 train_loss 0.012665 val_loss 0.001209 step_loss 0.867840 box_mae 0.001388 n_unstable 7 lr 1.39e-05\n",
      "Epoch 76 train_loss 0.012608 val_loss 0.001199 step_loss 0.838631 box_mae 0.001471 n_unstable 6 lr 1.39e-05\n",
      "Epoch 77 train_loss 0.012553 val_loss 0.001196 step_loss 0.540738 box_mae 0.001209 n_unstable 2 lr 1.39e-05\n",
      "Epoch 78 train_loss 0.012499 val_loss 0.001182 step_loss 0.414147 box_mae 0.001227 n_unstable 3 lr 1.39e-05\n",
      "Epoch 79 train_loss 0.012445 val_loss 0.001178 step_loss 0.500494 box_mae 0.001142 n_unstable 2 lr 1.39e-05\n",
      "Epoch 80 train_loss 0.012395 val_loss 0.001168 step_loss 1.012361 box_mae 0.001341 n_unstable 7 lr 1.39e-05\n",
      "Epoch 81 train_loss 0.012344 val_loss 0.001176 step_loss 0.247593 box_mae 0.001119 n_unstable 1 lr 1.39e-05\n",
      "Epoch 82 train_loss 0.012296 val_loss 0.001168 step_loss 1.137643 box_mae 0.001143 n_unstable 6 lr 1.39e-05\n",
      "Epoch 83 train_loss 0.012249 val_loss 0.001154 step_loss 1.349416 box_mae 0.001856 n_unstable 13 lr 1.39e-05\n",
      "Epoch 84 train_loss 0.012202 val_loss 0.001144 step_loss 0.299259 box_mae 0.001051 n_unstable 2 lr 1.39e-05\n",
      "Epoch 85 train_loss 0.012157 val_loss 0.001146 step_loss 0.535641 box_mae 0.001167 n_unstable 3 lr 1.39e-05\n",
      "Epoch 86 train_loss 0.012113 val_loss 0.001137 step_loss 1.225512 box_mae 0.001396 n_unstable 15 lr 1.39e-05\n",
      "Epoch 87 train_loss 0.012070 val_loss 0.001136 step_loss 0.533153 box_mae 0.001043 n_unstable 4 lr 1.39e-05\n",
      "Epoch 88 train_loss 0.012029 val_loss 0.001126 step_loss 0.726988 box_mae 0.001131 n_unstable 4 lr 1.39e-05\n",
      "Epoch 89 train_loss 0.011989 val_loss 0.001119 step_loss 0.681187 box_mae 0.001132 n_unstable 3 lr 1.39e-05\n",
      "Epoch 90 train_loss 0.011948 val_loss 0.001119 step_loss 0.835765 box_mae 0.001160 n_unstable 8 lr 1.39e-05\n",
      "Epoch 91 train_loss 0.011910 val_loss 0.001139 step_loss 0.689996 box_mae 0.001141 n_unstable 4 lr 1.39e-05\n",
      "Epoch 92 train_loss 0.011873 val_loss 0.001105 step_loss 0.453241 box_mae 0.001011 n_unstable 3 lr 1.39e-05\n",
      "Epoch 93 train_loss 0.011835 val_loss 0.001109 step_loss 0.544442 box_mae 0.000987 n_unstable 4 lr 1.39e-05\n",
      "Epoch 94 train_loss 0.011800 val_loss 0.001086 step_loss 0.622548 box_mae 0.001148 n_unstable 4 lr 1.39e-05\n",
      "Epoch 95 train_loss 0.011765 val_loss 0.001091 step_loss 0.390218 box_mae 0.001061 n_unstable 2 lr 1.39e-05\n",
      "Epoch 96 train_loss 0.011731 val_loss 0.001104 step_loss 0.319051 box_mae 0.001069 n_unstable 1 lr 1.39e-05\n",
      "Epoch 97 train_loss 0.011698 val_loss 0.001092 step_loss 0.383112 box_mae 0.001038 n_unstable 4 lr 1.39e-05\n",
      "Epoch 98 train_loss 0.011665 val_loss 0.001085 step_loss 0.378249 box_mae 0.001020 n_unstable 3 lr 1.39e-05\n",
      "Epoch 99 train_loss 0.011635 val_loss 0.001083 step_loss 0.445392 box_mae 0.001206 n_unstable 5 lr 1.39e-05\n",
      "Epoch 100 train_loss 0.011605 val_loss 0.001068 step_loss 0.229372 box_mae 0.000990 n_unstable 2 lr 1.39e-05\n",
      "Epoch 101 train_loss 0.011574 val_loss 0.001055 step_loss 0.348227 box_mae 0.001031 n_unstable 2 lr 1.39e-05\n",
      "Epoch 102 train_loss 0.011544 val_loss 0.001057 step_loss 0.477401 box_mae 0.001088 n_unstable 2 lr 1.39e-05\n",
      "Epoch 103 train_loss 0.011516 val_loss 0.001058 step_loss 0.399664 box_mae 0.001352 n_unstable 6 lr 1.39e-05\n",
      "Epoch 104 train_loss 0.011487 val_loss 0.001065 step_loss 0.368508 box_mae 0.000927 n_unstable 1 lr 1.39e-05\n",
      "Epoch 105 train_loss 0.011459 val_loss 0.001048 step_loss 0.337264 box_mae 0.000949 n_unstable 4 lr 1.39e-05\n",
      "Epoch 106 train_loss 0.011432 val_loss 0.001058 step_loss 0.315157 box_mae 0.000925 n_unstable 2 lr 1.39e-05\n",
      "Epoch 107 train_loss 0.011406 val_loss 0.001049 step_loss 0.270097 box_mae 0.001095 n_unstable 3 lr 1.39e-05\n",
      "Epoch 108 train_loss 0.011379 val_loss 0.001051 step_loss 0.252017 box_mae 0.001014 n_unstable 2 lr 1.39e-05\n",
      "Epoch 109 train_loss 0.011353 val_loss 0.001035 step_loss 0.325864 box_mae 0.000899 n_unstable 1 lr 1.39e-05\n",
      "Epoch 110 train_loss 0.011329 val_loss 0.001033 step_loss 0.212930 box_mae 0.000974 n_unstable 1 lr 1.39e-05\n",
      "Epoch 111 train_loss 0.011304 val_loss 0.001038 step_loss 0.257523 box_mae 0.001014 n_unstable 2 lr 1.39e-05\n",
      "Epoch 112 train_loss 0.011280 val_loss 0.001025 step_loss 0.242693 box_mae 0.001006 n_unstable 2 lr 1.39e-05\n",
      "Epoch 113 train_loss 0.011257 val_loss 0.001025 step_loss 0.398762 box_mae 0.001155 n_unstable 5 lr 1.39e-05\n",
      "Epoch 114 train_loss 0.011235 val_loss 0.001025 step_loss 0.219141 box_mae 0.000951 n_unstable 1 lr 1.39e-05\n",
      "Epoch 115 train_loss 0.011212 val_loss 0.001021 step_loss 0.190017 box_mae 0.000923 n_unstable 1 lr 1.39e-05\n",
      "Epoch 116 train_loss 0.011191 val_loss 0.001013 step_loss 0.228765 box_mae 0.001063 n_unstable 3 lr 1.39e-05\n",
      "Epoch 117 train_loss 0.011168 val_loss 0.001014 step_loss 0.193891 box_mae 0.000933 n_unstable 1 lr 1.39e-05\n",
      "Epoch 118 train_loss 0.011149 val_loss 0.001025 step_loss 0.210373 box_mae 0.000979 n_unstable 2 lr 1.39e-05\n",
      "Epoch 119 train_loss 0.011127 val_loss 0.001019 step_loss 0.667424 box_mae 0.001220 n_unstable 9 lr 1.39e-05\n",
      "Epoch 120 train_loss 0.011107 val_loss 0.001013 step_loss 0.232990 box_mae 0.001044 n_unstable 2 lr 1.39e-05\n",
      "Epoch 121 train_loss 0.011088 val_loss 0.001012 step_loss 0.441838 box_mae 0.001030 n_unstable 7 lr 1.39e-05\n",
      "Epoch 122 train_loss 0.011069 val_loss 0.001010 step_loss 0.257586 box_mae 0.001107 n_unstable 2 lr 1.39e-05\n",
      "Epoch 123 train_loss 0.011051 val_loss 0.001023 step_loss 0.227857 box_mae 0.001065 n_unstable 0 lr 1.39e-05\n",
      "Epoch 124 train_loss 0.011031 val_loss 0.000993 step_loss 0.253200 box_mae 0.001018 n_unstable 2 lr 1.39e-05\n",
      "Epoch 125 train_loss 0.011013 val_loss 0.000993 step_loss 0.185862 box_mae 0.000891 n_unstable 0 lr 1.39e-05\n",
      "Epoch 126 train_loss 0.010995 val_loss 0.000992 step_loss 0.399385 box_mae 0.001063 n_unstable 2 lr 1.39e-05\n",
      "Epoch 127 train_loss 0.010978 val_loss 0.000990 step_loss 0.304612 box_mae 0.001087 n_unstable 2 lr 1.39e-05\n",
      "Epoch 128 train_loss 0.010961 val_loss 0.000999 step_loss 0.199730 box_mae 0.000962 n_unstable 0 lr 1.39e-05\n",
      "Epoch 129 train_loss 0.010944 val_loss 0.000974 step_loss 0.199356 box_mae 0.000947 n_unstable 1 lr 1.39e-05\n",
      "Epoch 130 train_loss 0.010926 val_loss 0.000989 step_loss 0.209176 box_mae 0.000929 n_unstable 1 lr 1.39e-05\n",
      "Epoch 131 train_loss 0.010911 val_loss 0.000992 step_loss 0.192967 box_mae 0.000951 n_unstable 0 lr 1.39e-05\n",
      "Epoch 132 train_loss 0.010894 val_loss 0.001001 step_loss 0.211302 box_mae 0.001063 n_unstable 0 lr 1.39e-05\n",
      "Epoch 133 train_loss 0.010878 val_loss 0.000987 step_loss 0.206649 box_mae 0.000966 n_unstable 1 lr 1.39e-05\n",
      "Epoch 134 train_loss 0.010863 val_loss 0.000979 step_loss 0.191833 box_mae 0.000972 n_unstable 0 lr 1.39e-05\n",
      "Epoch 135 train_loss 0.010847 val_loss 0.000974 step_loss 0.191376 box_mae 0.000951 n_unstable 0 lr 1.39e-05\n",
      "Epoch 136 train_loss 0.010833 val_loss 0.000980 step_loss 0.221663 box_mae 0.000962 n_unstable 1 lr 1.39e-05\n",
      "Epoch 137 train_loss 0.010817 val_loss 0.000976 step_loss 0.182008 box_mae 0.000832 n_unstable 1 lr 1.39e-05\n",
      "Epoch 138 train_loss 0.010803 val_loss 0.000968 step_loss 0.179551 box_mae 0.000896 n_unstable 0 lr 1.39e-05\n",
      "Epoch 139 train_loss 0.010787 val_loss 0.000981 step_loss 0.182949 box_mae 0.000896 n_unstable 0 lr 1.39e-05\n",
      "Epoch 140 train_loss 0.010774 val_loss 0.000983 step_loss 0.198825 box_mae 0.000989 n_unstable 0 lr 1.39e-05\n",
      "Epoch 141 train_loss 0.010759 val_loss 0.000963 step_loss 0.179886 box_mae 0.000881 n_unstable 0 lr 1.39e-05\n",
      "Epoch 142 train_loss 0.010745 val_loss 0.000969 step_loss 0.187626 box_mae 0.000917 n_unstable 0 lr 1.39e-05\n",
      "Epoch 143 train_loss 0.010731 val_loss 0.000967 step_loss 0.192760 box_mae 0.000981 n_unstable 0 lr 1.39e-05\n",
      "Epoch 144 train_loss 0.010717 val_loss 0.000956 step_loss 0.167719 box_mae 0.000818 n_unstable 0 lr 1.39e-05\n",
      "Epoch 145 train_loss 0.010703 val_loss 0.000963 step_loss 0.176445 box_mae 0.000903 n_unstable 0 lr 1.39e-05\n",
      "Epoch 146 train_loss 0.010691 val_loss 0.000973 step_loss 0.196930 box_mae 0.001119 n_unstable 0 lr 1.39e-05\n",
      "Epoch 147 train_loss 0.010678 val_loss 0.000967 step_loss 0.207459 box_mae 0.001160 n_unstable 1 lr 1.39e-05\n",
      "Epoch 148 train_loss 0.010665 val_loss 0.000961 step_loss 0.168196 box_mae 0.000835 n_unstable 0 lr 1.39e-05\n",
      "Epoch 149 train_loss 0.010652 val_loss 0.000957 step_loss 0.173328 box_mae 0.000847 n_unstable 0 lr 1.39e-05\n",
      "Epoch 150 train_loss 0.010640 val_loss 0.000957 step_loss 0.169071 box_mae 0.000833 n_unstable 0 lr 1.39e-05\n",
      "Epoch 151 train_loss 0.010626 val_loss 0.000953 step_loss 0.225449 box_mae 0.001073 n_unstable 1 lr 1.39e-05\n",
      "Epoch 152 train_loss 0.010615 val_loss 0.000969 step_loss 0.193463 box_mae 0.000954 n_unstable 0 lr 1.39e-05\n",
      "Epoch 153 train_loss 0.010602 val_loss 0.000962 step_loss 0.169856 box_mae 0.000853 n_unstable 0 lr 1.39e-05\n",
      "Epoch 154 train_loss 0.010591 val_loss 0.000961 step_loss 0.181396 box_mae 0.000973 n_unstable 0 lr 1.39e-05\n",
      "Epoch 155 train_loss 0.010577 val_loss 0.000961 step_loss 0.182180 box_mae 0.000894 n_unstable 0 lr 1.39e-05\n",
      "Epoch 156 train_loss 0.010567 val_loss 0.000951 step_loss 0.168668 box_mae 0.000850 n_unstable 0 lr 1.39e-05\n",
      "Epoch 157 train_loss 0.010555 val_loss 0.000967 step_loss 0.210833 box_mae 0.001098 n_unstable 1 lr 1.39e-05\n",
      "Epoch 158 train_loss 0.010543 val_loss 0.000966 step_loss 0.193217 box_mae 0.000953 n_unstable 0 lr 1.39e-05\n",
      "Epoch 159 train_loss 0.010532 val_loss 0.000952 step_loss 0.168169 box_mae 0.000836 n_unstable 0 lr 1.39e-05\n",
      "Epoch 160 train_loss 0.010520 val_loss 0.000953 step_loss 0.173871 box_mae 0.000876 n_unstable 0 lr 1.39e-05\n",
      "Epoch 161 train_loss 0.010509 val_loss 0.000953 step_loss 0.177407 box_mae 0.000890 n_unstable 0 lr 1.39e-05\n",
      "Epoch 162 train_loss 0.010499 val_loss 0.000943 step_loss 0.166104 box_mae 0.000844 n_unstable 0 lr 1.39e-05\n",
      "Epoch 163 train_loss 0.010488 val_loss 0.000950 step_loss 0.199947 box_mae 0.001131 n_unstable 1 lr 1.39e-05\n",
      "Epoch 164 train_loss 0.010476 val_loss 0.000950 step_loss 0.180702 box_mae 0.000883 n_unstable 0 lr 1.39e-05\n",
      "Epoch 165 train_loss 0.010465 val_loss 0.000942 step_loss 0.179003 box_mae 0.000938 n_unstable 0 lr 1.39e-05\n",
      "Epoch 166 train_loss 0.010456 val_loss 0.000945 step_loss 0.185704 box_mae 0.000996 n_unstable 0 lr 1.39e-05\n",
      "Epoch 167 train_loss 0.010444 val_loss 0.000946 step_loss 0.165329 box_mae 0.000800 n_unstable 0 lr 1.39e-05\n",
      "Epoch 168 train_loss 0.010435 val_loss 0.000952 step_loss 0.169252 box_mae 0.000845 n_unstable 0 lr 1.39e-05\n",
      "Epoch 169 train_loss 0.010424 val_loss 0.000943 step_loss 0.173138 box_mae 0.000850 n_unstable 0 lr 1.39e-05\n",
      "Epoch 170 train_loss 0.010413 val_loss 0.000946 step_loss 0.167887 box_mae 0.000823 n_unstable 0 lr 1.39e-05\n",
      "Epoch 171 train_loss 0.010403 val_loss 0.000936 step_loss 0.163899 box_mae 0.000817 n_unstable 0 lr 1.39e-05\n",
      "Epoch 172 train_loss 0.010393 val_loss 0.000937 step_loss 0.164447 box_mae 0.000817 n_unstable 0 lr 1.39e-05\n",
      "Epoch 173 train_loss 0.010383 val_loss 0.000940 step_loss 0.166264 box_mae 0.000814 n_unstable 0 lr 1.39e-05\n",
      "Epoch 174 train_loss 0.010372 val_loss 0.000953 step_loss 0.177496 box_mae 0.000888 n_unstable 0 lr 1.39e-05\n",
      "Epoch 175 train_loss 0.010363 val_loss 0.000953 step_loss 0.184069 box_mae 0.000937 n_unstable 0 lr 1.39e-05\n",
      "Epoch 176 train_loss 0.010352 val_loss 0.000936 step_loss 0.193156 box_mae 0.001103 n_unstable 0 lr 1.39e-05\n",
      "Epoch 177 train_loss 0.010342 val_loss 0.000964 step_loss 0.171592 box_mae 0.000901 n_unstable 0 lr 1.39e-05\n",
      "Epoch 178 train_loss 0.010334 val_loss 0.000940 step_loss 0.162163 box_mae 0.000780 n_unstable 0 lr 1.39e-05\n",
      "Epoch 179 train_loss 0.010323 val_loss 0.000933 step_loss 0.170717 box_mae 0.000882 n_unstable 0 lr 1.39e-05\n",
      "Epoch 180 train_loss 0.010314 val_loss 0.000949 step_loss 0.180368 box_mae 0.000878 n_unstable 0 lr 1.39e-05\n",
      "Epoch 181 train_loss 0.010304 val_loss 0.000946 step_loss 0.170655 box_mae 0.000856 n_unstable 0 lr 1.39e-05\n",
      "Epoch 182 train_loss 0.010296 val_loss 0.000937 step_loss 0.186257 box_mae 0.000978 n_unstable 0 lr 1.39e-05\n",
      "Epoch 183 train_loss 0.010287 val_loss 0.000946 step_loss 0.195924 box_mae 0.001080 n_unstable 0 lr 1.39e-05\n",
      "Epoch 184 train_loss 0.010278 val_loss 0.000941 step_loss 0.173544 box_mae 0.000816 n_unstable 0 lr 1.39e-05\n",
      "Epoch 185 train_loss 0.010270 val_loss 0.000936 step_loss 0.166748 box_mae 0.000818 n_unstable 0 lr 1.39e-05\n",
      "Epoch 186 train_loss 0.010260 val_loss 0.000936 step_loss 0.195021 box_mae 0.001175 n_unstable 0 lr 1.39e-05\n",
      "Epoch 187 train_loss 0.010252 val_loss 0.000938 step_loss 0.169613 box_mae 0.000844 n_unstable 0 lr 1.39e-05\n",
      "Epoch 188 train_loss 0.010243 val_loss 0.000933 step_loss 0.174393 box_mae 0.000864 n_unstable 0 lr 1.39e-05\n",
      "Epoch 189 train_loss 0.010235 val_loss 0.000930 step_loss 0.172878 box_mae 0.000873 n_unstable 0 lr 1.39e-05\n",
      "Epoch 190 train_loss 0.010226 val_loss 0.000935 step_loss 0.171042 box_mae 0.000860 n_unstable 0 lr 1.39e-05\n",
      "Epoch 191 train_loss 0.010218 val_loss 0.000936 step_loss 0.170968 box_mae 0.000876 n_unstable 0 lr 1.39e-05\n",
      "Epoch 192 train_loss 0.010210 val_loss 0.000929 step_loss 0.165276 box_mae 0.000815 n_unstable 0 lr 1.39e-05\n",
      "Epoch 193 train_loss 0.010201 val_loss 0.000925 step_loss 0.168240 box_mae 0.000890 n_unstable 0 lr 1.39e-05\n",
      "Epoch 194 train_loss 0.010194 val_loss 0.000942 step_loss 0.187031 box_mae 0.000911 n_unstable 0 lr 1.39e-05\n",
      "Epoch 195 train_loss 0.010186 val_loss 0.000931 step_loss 0.220887 box_mae 0.001054 n_unstable 1 lr 1.39e-05\n",
      "Epoch 196 train_loss 0.010177 val_loss 0.000930 step_loss 0.178588 box_mae 0.000944 n_unstable 0 lr 1.39e-05\n",
      "Epoch 197 train_loss 0.010168 val_loss 0.000931 step_loss 0.185674 box_mae 0.001000 n_unstable 0 lr 1.39e-05\n",
      "Epoch 198 train_loss 0.010161 val_loss 0.000927 step_loss 0.163431 box_mae 0.000808 n_unstable 0 lr 1.39e-05\n",
      "Epoch 199 train_loss 0.010154 val_loss 0.000930 step_loss 0.185692 box_mae 0.001047 n_unstable 0 lr 1.39e-05\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "for epoch in range(500):\n",
    "    \n",
    "    # Train in batch mode\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    train_loss = []\n",
    "    for k, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = nn.L1Loss()(y.to(device), model(x.to(device)))\n",
    "        \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        \n",
    "        loss += L1_penalty * l1_norm\n",
    "        loss += L2_penalty * l2_norm\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        #lr_scheduler.step()\n",
    "        \n",
    "    # Validate \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Validate in batch mode\n",
    "        valid_loss = []\n",
    "        for k, (x, y) in enumerate(valid_loader):\n",
    "            loss = nn.L1Loss()(y.to(device), model(x.to(device)))\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "        # Validate in box mode\n",
    "        box_loss = []\n",
    "        \n",
    "        # set up array for saving predicted results\n",
    "        _in_array = torch.from_numpy(val_in_array.copy()).float()\n",
    "        pred_array = np.empty((val_in_array.shape[0], num_timesteps-start_time, len(out_col_idx)))\n",
    "\n",
    "        # use initial condition @ t = start_time and get the first prediction\n",
    "        gamma = model(_in_array[:, start_time, :].to(device))\n",
    "        pred_array[:, 0, :] = gamma.cpu().numpy()\n",
    "        loss = nn.L1Loss()(_in_array[:, start_time + 1, out_col_idx], gamma.cpu()).item()\n",
    "        box_loss.append(loss)\n",
    "\n",
    "        # use the first prediction to get the next, and so on for num_timesteps\n",
    "        for k, i in enumerate(range(start_time + 1, num_timesteps)): \n",
    "            new_input = _in_array[:, i, :]\n",
    "            new_input[:, out_col_idx] = gamma.cpu()\n",
    "            gamma = model(new_input.to(device))\n",
    "            pred_array[:, k+1, :] = gamma.cpu().numpy()\n",
    "            if i < (num_timesteps-1):\n",
    "                loss = nn.L1Loss()(_in_array[:, i+1, out_col_idx], gamma.cpu()).item()\n",
    "                box_loss.append(loss)\n",
    "                \n",
    "        idx = transformed_data[\"val_out\"].index\n",
    "        start_time_units = sorted(list(set([x[0] for x in idx])))[start_time]\n",
    "        start_time_condition = [(x[0] >= start_time_units) for x in idx]\n",
    "        idx = transformed_data[\"val_out\"][start_time_condition].index\n",
    "\n",
    "        raw_box_preds = pd.DataFrame(\n",
    "            data=pred_array.reshape(-1, len(output_vars)),\n",
    "            columns=output_vars, \n",
    "            index=idx\n",
    "        )\n",
    "\n",
    "        # inverse transform \n",
    "        truth, preds = inv_transform_preds(\n",
    "            raw_preds=raw_box_preds,\n",
    "            truth=data['val_out'][start_time_condition],\n",
    "            y_scaler=y_scaler,\n",
    "            log_trans_cols=log_trans_cols,\n",
    "            tendency_cols=tendency_cols)\n",
    "                \n",
    "        metrics = ensembled_metrics(y_true=truth,\n",
    "                                    y_pred=preds,\n",
    "                                    member=0,\n",
    "                                    output_vars=output_vars,\n",
    "                                    stability_thresh=1.0)\n",
    "        mean_box_mae = metrics['mean_mae'].mean()\n",
    "        unstable_exps = metrics['n_unstable'].mean()\n",
    "        \n",
    "    results_dict[\"epoch\"].append(epoch)\n",
    "    results_dict[\"train_loss\"].append(np.mean(train_loss))\n",
    "    results_dict[\"val_loss\"].append(np.mean(valid_loss))\n",
    "    results_dict[\"step_loss\"].append(np.mean(box_loss))\n",
    "    results_dict[\"box_mae\"].append(mean_box_mae)\n",
    "    results_dict[\"n_unstable\"].append(unstable_exps)\n",
    "    results_dict[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save the dataframe to disk\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "    df.to_csv(f\"gecko/training_log_01.csv\", index = False)\n",
    "    \n",
    "    print(f'Epoch {epoch}',\n",
    "          f'train_loss {results_dict[\"train_loss\"][-1]:4f}', \n",
    "          f'val_loss {results_dict[\"val_loss\"][-1]:4f}',\n",
    "          f'step_loss {results_dict[\"step_loss\"][-1]:4f}',\n",
    "          f'box_mae {results_dict[\"box_mae\"][-1]:4f}',\n",
    "          f'n_unstable {int(results_dict[\"n_unstable\"][-1])}',\n",
    "          f'lr {results_dict[\"lr\"][-1]}'\n",
    "         )\n",
    "\n",
    "    #anneal the learning rate using just the box metric\n",
    "    #lr_scheduler.step()\n",
    "    \n",
    "    if results_dict[\"box_mae\"][-1] == min(results_dict[\"box_mae\"]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': min(results_dict[\"box_mae\"])\n",
    "        }\n",
    "        torch.save(state_dict, f\"gecko/mlp_01.pt\")\n",
    "    \n",
    "    # Stop training if we have not improved after X epochs\n",
    "#     best_epoch = [i for i,j in enumerate(results_dict[\"box_mae\"]) if j == min(results_dict[\"box_mae\"])][0]\n",
    "#     offset = epoch - best_epoch\n",
    "#     if offset >= stopping_patience:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train keras model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model = Sequential()\n",
    "# keras_model.add(Dense(middle_size, input_dim=input_size, activation=\"relu\", \n",
    "#                       kernel_regularizer=tf.keras.regularizers.l1_l2(l1=L1_penalty, l2=L2_penalty)))\n",
    "# keras_model.add(Dense(output_size, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_model.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.fit(\n",
    "#     transformed_data[\"train_in\"],\n",
    "#     transformed_data[\"train_out\"],\n",
    "#     validation_data = (transformed_data[\"val_in\"], transformed_data[\"val_out\"]),\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True,\n",
    "#     epochs = 841,\n",
    "#     verbose = 2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validate in box mode\n",
    "# box_loss = []\n",
    "\n",
    "# # set up array for saving predicted results\n",
    "# _in_array = val_in_array.copy()\n",
    "# pred_array = np.empty((val_in_array.shape[0], num_timesteps-start_time, len(out_col_idx)))\n",
    "\n",
    "# # use initial condition @ t = start_time and get the first prediction\n",
    "# gamma = keras_model.predict(_in_array[:, start_time, :])\n",
    "# pred_array[:, 0, :] = gamma\n",
    "\n",
    "# # use the first prediction to get the next, and so on for num_timesteps\n",
    "# for k, i in enumerate(range(start_time + 1, num_timesteps)): \n",
    "#     new_input = _in_array[:, i, :]\n",
    "#     new_input[:, out_col_idx] = gamma\n",
    "#     gamma = keras_model.predict(new_input)\n",
    "#     pred_array[:, k+1, :] = gamma\n",
    "\n",
    "# idx = transformed_data[\"val_out\"].index\n",
    "# start_time_units = sorted(list(set([x[0] for x in idx])))[start_time]\n",
    "# start_time_condition = [(x[0] >= start_time_units) for x in idx]\n",
    "# idx = transformed_data[\"val_out\"][start_time_condition].index\n",
    "\n",
    "# raw_box_preds = pd.DataFrame(\n",
    "#     data=pred_array.reshape(-1, len(output_vars)),\n",
    "#     columns=output_vars, \n",
    "#     index=idx\n",
    "# )\n",
    "\n",
    "# # inverse transform \n",
    "# truth, preds = inv_transform_preds(\n",
    "#     raw_preds=raw_box_preds,\n",
    "#     truth=data['val_out'][start_time_condition],\n",
    "#     y_scaler=y_scaler,\n",
    "#     log_trans_cols=log_trans_cols,\n",
    "#     tendency_cols=tendency_cols)\n",
    "\n",
    "# metrics = ensembled_metrics(y_true=truth,\n",
    "#                             y_pred=preds,\n",
    "#                             member=0,\n",
    "#                             output_vars=output_vars,\n",
    "#                             stability_thresh=1.0)\n",
    "# mean_box_mae = metrics['mean_mae'].mean()\n",
    "# unstable_exps = metrics['n_unstable'].mean()\n",
    "\n",
    "# print(mean_box_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can the VQ-VAE reconstruct the data, using FCL for encoder/decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size = 9, output_size = 100, fcl_layers = 1, dr = 0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fcn = self.make_fcn(input_size, output_size, fcl_layers, dr)\n",
    "\n",
    "    \n",
    "    def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "        if len(fcl_layers) > 0:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, fcl_layers[0]),\n",
    "                #nn.BatchNorm1d(fcl_layers[0]),\n",
    "                #nn.Dropout(dr),\n",
    "                torch.nn.ReLU()\n",
    "            ]\n",
    "            if len(fcl_layers) == 1:\n",
    "                fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "            else:\n",
    "                for i in range(len(fcl_layers)-1):\n",
    "                    fcn += [\n",
    "                        nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "                        #nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        #nn.Dropout(dr)\n",
    "                    ]\n",
    "                #fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "        else:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, output_size)\n",
    "            ]\n",
    "        return nn.Sequential(*fcn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fcn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size  = 100, output_size = 3, fcl_layers = 1, dr = 0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fcn = self.make_fcn(input_size, output_size, fcl_layers, dr)\n",
    "\n",
    "    def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "        if len(fcl_layers) > 0:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, fcl_layers[0]),\n",
    "                #nn.BatchNorm1d(fcl_layers[0]),\n",
    "                torch.nn.ReLU(),\n",
    "                #nn.Dropout(dr)\n",
    "            ]\n",
    "            if len(fcl_layers) == 1:\n",
    "                fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "            else:\n",
    "                for i in range(len(fcl_layers)-1):\n",
    "                    fcn += [\n",
    "                        nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "                        #nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        #nn.Dropout(dr)\n",
    "                    ]\n",
    "                #fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "        else:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, output_size)\n",
    "            ]\n",
    "        return nn.Sequential(*fcn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fcn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n",
    "    \n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        flat_input = inputs.contiguous()\n",
    "        #inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        #flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, middle_size, output_size, fcl_layers, dr,\n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(input_size, embedding_dim, fcl_layers, dr)\n",
    "        \n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "            \n",
    "        self._decoder = Decoder(embedding_dim, output_size, fcl_layers[::-1], dr)\n",
    "        \n",
    "#         self.fc1 = torch.nn.Linear(input_size + output_size, middle_size)\n",
    "#         #self.bn1 = nn.BatchNorm1d(middle_size)\n",
    "#         self.ac1 = nn.ReLU()\n",
    "#         #self.dr1 = nn.Dropout(dr)\n",
    "#         self.fc2 = torch.nn.Linear(middle_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "#         x = torch.cat([x, x_recon], axis = 1)\n",
    "#         x = self.fc1(x)\n",
    "#         #x = self.bn1(x)\n",
    "#         x = self.ac1(x)\n",
    "#         #x = self.dr1(x)\n",
    "#         x = self.fc2(x)\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 9\n",
    "middle_size = 128\n",
    "output_size = 9\n",
    "\n",
    "fcl_layers = []\n",
    "embedding_dim = middle_size\n",
    "num_embeddings = 512\n",
    "\n",
    "dr = 0.0\n",
    "commitment_cost = 0.25\n",
    "decay = 0.99\n",
    "loss_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = Model(input_size, middle_size, 3, \n",
    "              fcl_layers, dr, num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "\n",
    "vae_model = vae_model.apply(initialize_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (_encoder): Encoder(\n",
       "    (fcn): Sequential(\n",
       "      (0): Linear(in_features=9, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_vq_vae): VectorQuantizerEMA(\n",
       "    (_embedding): Embedding(512, 128)\n",
       "  )\n",
       "  (_decoder): Decoder(\n",
       "    (fcn): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae_model.parameters(),\n",
    "                             lr = 1e-3,\n",
    "                             eps = 1e-7,\n",
    "                             betas = (0.9, 0.999),\n",
    "                             amsgrad = False)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer, \n",
    "#     T_max=10, \n",
    "#     eta_min=1e-3*learning_rate\n",
    "# )\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        patience = lr_patience, \n",
    "        verbose = True,\n",
    "        min_lr = 1.0e-13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.222003 train_mse 0.193752 train_perp 4.848610 valid_loss 0.486008 valid_mae 0.446790 valid_perp 4.032362 lr 0.001000\n",
      "Epoch 1 train_loss 0.210637 train_mse 0.159421 train_perp 4.739242 valid_loss 0.506191 valid_mae 0.438513 valid_perp 4.017611 lr 0.001000\n",
      "Epoch 2 train_loss 0.188017 train_mse 0.133635 train_perp 8.431925 valid_loss 0.344203 valid_mae 0.309318 valid_perp 11.241926 lr 0.001000\n",
      "Epoch 3 train_loss 0.076186 train_mse 0.055412 train_perp 48.337995 valid_loss 0.223215 valid_mae 0.207162 valid_perp 22.814142 lr 0.001000\n",
      "Epoch 4 train_loss 0.048049 train_mse 0.035123 train_perp 79.883409 valid_loss 0.189214 valid_mae 0.177309 valid_perp 28.421633 lr 0.001000\n",
      "Epoch 5 train_loss 0.037072 train_mse 0.027088 train_perp 99.291526 valid_loss 0.169258 valid_mae 0.159621 valid_perp 31.669755 lr 0.001000\n",
      "Epoch 6 train_loss 0.030644 train_mse 0.022349 train_perp 114.888693 valid_loss 0.152940 valid_mae 0.144810 valid_perp 34.782575 lr 0.001000\n",
      "Epoch 7 train_loss 0.026235 train_mse 0.019039 train_perp 127.630508 valid_loss 0.140252 valid_mae 0.133212 valid_perp 38.074864 lr 0.001000\n",
      "Epoch 8 train_loss 0.022606 train_mse 0.016309 train_perp 138.481071 valid_loss 0.130414 valid_mae 0.124205 valid_perp 41.499291 lr 0.001000\n",
      "Epoch 9 train_loss 0.019959 train_mse 0.014368 train_perp 148.624818 valid_loss 0.123034 valid_mae 0.117436 valid_perp 44.680715 lr 0.001000\n",
      "Epoch 10 train_loss 0.018095 train_mse 0.012991 train_perp 157.133476 valid_loss 0.114910 valid_mae 0.109757 valid_perp 46.426541 lr 0.001000\n",
      "Epoch 11 train_loss 0.016829 train_mse 0.012014 train_perp 163.575455 valid_loss 0.109931 valid_mae 0.105044 valid_perp 47.690229 lr 0.001000\n",
      "Epoch 12 train_loss 0.015891 train_mse 0.011258 train_perp 168.749236 valid_loss 0.107078 valid_mae 0.102307 valid_perp 48.877101 lr 0.001000\n",
      "Epoch 13 train_loss 0.015119 train_mse 0.010626 train_perp 173.490448 valid_loss 0.106167 valid_mae 0.101480 valid_perp 50.637414 lr 0.001000\n",
      "Epoch 14 train_loss 0.014498 train_mse 0.010110 train_perp 177.036892 valid_loss 0.103251 valid_mae 0.098609 valid_perp 52.123162 lr 0.001000\n",
      "Epoch 15 train_loss 0.014052 train_mse 0.009708 train_perp 179.255330 valid_loss 0.103725 valid_mae 0.099086 valid_perp 52.400112 lr 0.001000\n",
      "Epoch 16 train_loss 0.013718 train_mse 0.009382 train_perp 181.148555 valid_loss 0.101697 valid_mae 0.097044 valid_perp 52.473046 lr 0.001000\n",
      "Epoch 17 train_loss 0.013487 train_mse 0.009120 train_perp 182.758438 valid_loss 0.101451 valid_mae 0.096748 valid_perp 52.567319 lr 0.001000\n",
      "Epoch 18 train_loss 0.013341 train_mse 0.008913 train_perp 184.640794 valid_loss 0.098707 valid_mae 0.093956 valid_perp 52.648730 lr 0.001000\n",
      "Epoch 19 train_loss 0.013259 train_mse 0.008750 train_perp 186.637392 valid_loss 0.097868 valid_mae 0.093048 valid_perp 52.696473 lr 0.001000\n",
      "Epoch 20 train_loss 0.013195 train_mse 0.008598 train_perp 188.767143 valid_loss 0.097409 valid_mae 0.092518 valid_perp 52.738683 lr 0.001000\n",
      "Epoch 21 train_loss 0.013104 train_mse 0.008434 train_perp 190.910238 valid_loss 0.096426 valid_mae 0.091439 valid_perp 53.040849 lr 0.001000\n",
      "Epoch 22 train_loss 0.013054 train_mse 0.008302 train_perp 193.203481 valid_loss 0.096485 valid_mae 0.091366 valid_perp 53.266555 lr 0.001000\n",
      "Epoch 23 train_loss 0.013017 train_mse 0.008184 train_perp 195.362142 valid_loss 0.096158 valid_mae 0.090913 valid_perp 53.763515 lr 0.001000\n",
      "Epoch 24 train_loss 0.013011 train_mse 0.008088 train_perp 197.261917 valid_loss 0.096107 valid_mae 0.090753 valid_perp 54.052749 lr 0.001000\n",
      "Epoch 25 train_loss 0.012970 train_mse 0.007973 train_perp 199.121807 valid_loss 0.095198 valid_mae 0.089775 valid_perp 54.623560 lr 0.001000\n",
      "Epoch 26 train_loss 0.012887 train_mse 0.007849 train_perp 201.445358 valid_loss 0.095670 valid_mae 0.090200 valid_perp 55.423722 lr 0.001000\n",
      "Epoch 27 train_loss 0.012758 train_mse 0.007708 train_perp 204.190769 valid_loss 0.096392 valid_mae 0.090900 valid_perp 56.159881 lr 0.001000\n",
      "Epoch 28 train_loss 0.012656 train_mse 0.007584 train_perp 206.708677 valid_loss 0.094574 valid_mae 0.089060 valid_perp 56.927318 lr 0.001000\n",
      "Epoch 29 train_loss 0.012521 train_mse 0.007450 train_perp 209.091969 valid_loss 0.093526 valid_mae 0.088034 valid_perp 57.171735 lr 0.001000\n",
      "Epoch 30 train_loss 0.012353 train_mse 0.007301 train_perp 211.369589 valid_loss 0.092726 valid_mae 0.087224 valid_perp 57.706954 lr 0.001000\n",
      "Epoch 31 train_loss 0.012181 train_mse 0.007156 train_perp 214.008302 valid_loss 0.092107 valid_mae 0.086621 valid_perp 58.218990 lr 0.001000\n",
      "Epoch 32 train_loss 0.012067 train_mse 0.007045 train_perp 216.730456 valid_loss 0.090968 valid_mae 0.085471 valid_perp 58.596487 lr 0.001000\n",
      "Epoch 33 train_loss 0.011992 train_mse 0.006957 train_perp 219.466538 valid_loss 0.092124 valid_mae 0.086613 valid_perp 59.081257 lr 0.001000\n",
      "Epoch 34 train_loss 0.011913 train_mse 0.006873 train_perp 222.452891 valid_loss 0.092472 valid_mae 0.086940 valid_perp 59.490367 lr 0.001000\n",
      "Epoch 35 train_loss 0.011825 train_mse 0.006785 train_perp 225.300238 valid_loss 0.091506 valid_mae 0.085975 valid_perp 59.659478 lr 0.001000\n",
      "Epoch 36 train_loss 0.011746 train_mse 0.006707 train_perp 227.799326 valid_loss 0.090746 valid_mae 0.085223 valid_perp 60.066334 lr 0.001000\n",
      "Epoch 37 train_loss 0.011691 train_mse 0.006644 train_perp 230.034574 valid_loss 0.089544 valid_mae 0.084012 valid_perp 60.395268 lr 0.001000\n",
      "Epoch 38 train_loss 0.011610 train_mse 0.006567 train_perp 232.149870 valid_loss 0.089269 valid_mae 0.083747 valid_perp 60.619549 lr 0.001000\n",
      "Epoch 39 train_loss 0.011517 train_mse 0.006490 train_perp 234.110176 valid_loss 0.088811 valid_mae 0.083306 valid_perp 61.191640 lr 0.001000\n",
      "Epoch 40 train_loss 0.011419 train_mse 0.006414 train_perp 236.293613 valid_loss 0.088460 valid_mae 0.082981 valid_perp 61.816637 lr 0.001000\n",
      "Epoch 41 train_loss 0.011304 train_mse 0.006332 train_perp 238.604247 valid_loss 0.087467 valid_mae 0.082028 valid_perp 62.238619 lr 0.001000\n",
      "Epoch 42 train_loss 0.011170 train_mse 0.006249 train_perp 241.248723 valid_loss 0.088494 valid_mae 0.083060 valid_perp 62.531058 lr 0.001000\n",
      "Epoch 43 train_loss 0.011033 train_mse 0.006161 train_perp 243.805471 valid_loss 0.087800 valid_mae 0.082406 valid_perp 63.031531 lr 0.001000\n",
      "Epoch 44 train_loss 0.010940 train_mse 0.006099 train_perp 245.756268 valid_loss 0.087820 valid_mae 0.082489 valid_perp 63.345110 lr 0.001000\n",
      "Epoch 45 train_loss 0.010831 train_mse 0.006023 train_perp 247.936206 valid_loss 0.087525 valid_mae 0.082196 valid_perp 63.723937 lr 0.001000\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 46 train_loss 0.010703 train_mse 0.005933 train_perp 250.905533 valid_loss 0.086139 valid_mae 0.080839 valid_perp 64.087498 lr 0.000100\n",
      "Epoch 47 train_loss 0.010602 train_mse 0.005872 train_perp 253.334183 valid_loss 0.085942 valid_mae 0.080652 valid_perp 64.143087 lr 0.000100\n",
      "Epoch 48 train_loss 0.010534 train_mse 0.005825 train_perp 255.087472 valid_loss 0.085524 valid_mae 0.080260 valid_perp 64.418269 lr 0.000100\n",
      "Epoch 49 train_loss 0.010475 train_mse 0.005782 train_perp 256.609680 valid_loss 0.085405 valid_mae 0.080163 valid_perp 64.500595 lr 0.000100\n",
      "Epoch 50 train_loss 0.010416 train_mse 0.005740 train_perp 258.444906 valid_loss 0.085146 valid_mae 0.079914 valid_perp 64.671921 lr 0.000100\n",
      "Epoch 51 train_loss 0.010353 train_mse 0.005695 train_perp 259.815852 valid_loss 0.084871 valid_mae 0.079628 valid_perp 64.680736 lr 0.000100\n",
      "Epoch 52 train_loss 0.010307 train_mse 0.005660 train_perp 261.043111 valid_loss 0.085040 valid_mae 0.079789 valid_perp 65.082472 lr 0.000100\n",
      "Epoch 53 train_loss 0.010262 train_mse 0.005623 train_perp 262.431387 valid_loss 0.085033 valid_mae 0.079780 valid_perp 65.322469 lr 0.000100\n",
      "Epoch 54 train_loss 0.010217 train_mse 0.005587 train_perp 263.501081 valid_loss 0.084484 valid_mae 0.079233 valid_perp 65.457908 lr 0.000100\n",
      "Epoch 55 train_loss 0.010174 train_mse 0.005553 train_perp 264.576623 valid_loss 0.084188 valid_mae 0.078934 valid_perp 65.733743 lr 0.000100\n",
      "Epoch 56 train_loss 0.010140 train_mse 0.005524 train_perp 266.338662 valid_loss 0.084129 valid_mae 0.078887 valid_perp 66.196393 lr 0.000100\n",
      "Epoch 57 train_loss 0.010111 train_mse 0.005496 train_perp 267.811157 valid_loss 0.083815 valid_mae 0.078584 valid_perp 66.367646 lr 0.000100\n",
      "Epoch 58 train_loss 0.010087 train_mse 0.005473 train_perp 268.916696 valid_loss 0.083819 valid_mae 0.078589 valid_perp 66.436555 lr 0.000100\n",
      "Epoch 59 train_loss 0.010058 train_mse 0.005449 train_perp 269.873839 valid_loss 0.083685 valid_mae 0.078450 valid_perp 66.593324 lr 0.000100\n",
      "Epoch 60 train_loss 0.010028 train_mse 0.005422 train_perp 270.863889 valid_loss 0.083762 valid_mae 0.078516 valid_perp 66.773039 lr 0.000100\n",
      "Epoch 61 train_loss 0.009984 train_mse 0.005391 train_perp 272.243551 valid_loss 0.083512 valid_mae 0.078271 valid_perp 67.042067 lr 0.000100\n",
      "Epoch 62 train_loss 0.009932 train_mse 0.005356 train_perp 273.917381 valid_loss 0.083527 valid_mae 0.078300 valid_perp 67.316793 lr 0.000100\n",
      "Epoch 63 train_loss 0.009882 train_mse 0.005323 train_perp 275.748153 valid_loss 0.083158 valid_mae 0.077948 valid_perp 67.536967 lr 0.000100\n",
      "Epoch 64 train_loss 0.009814 train_mse 0.005281 train_perp 277.509641 valid_loss 0.083302 valid_mae 0.078139 valid_perp 67.772593 lr 0.000100\n",
      "Epoch 65 train_loss 0.009744 train_mse 0.005240 train_perp 279.198202 valid_loss 0.082476 valid_mae 0.077326 valid_perp 68.131292 lr 0.000100\n",
      "Epoch 66 train_loss 0.009692 train_mse 0.005206 train_perp 281.245085 valid_loss 0.082083 valid_mae 0.076954 valid_perp 68.402481 lr 0.000100\n",
      "Epoch 67 train_loss 0.009644 train_mse 0.005175 train_perp 282.791693 valid_loss 0.081796 valid_mae 0.076703 valid_perp 68.413969 lr 0.000100\n",
      "Epoch 68 train_loss 0.009609 train_mse 0.005150 train_perp 283.925582 valid_loss 0.082116 valid_mae 0.077026 valid_perp 68.457817 lr 0.000100\n",
      "Epoch 69 train_loss 0.009578 train_mse 0.005127 train_perp 285.266835 valid_loss 0.081605 valid_mae 0.076533 valid_perp 68.626280 lr 0.000100\n",
      "Epoch 70 train_loss 0.009538 train_mse 0.005101 train_perp 286.502234 valid_loss 0.081690 valid_mae 0.076628 valid_perp 69.057838 lr 0.000100\n",
      "Epoch 71 train_loss 0.009513 train_mse 0.005080 train_perp 287.464938 valid_loss 0.081203 valid_mae 0.076141 valid_perp 69.320276 lr 0.000100\n",
      "Epoch 72 train_loss 0.009496 train_mse 0.005065 train_perp 288.122813 valid_loss 0.081141 valid_mae 0.076083 valid_perp 69.244854 lr 0.000100\n",
      "Epoch 73 train_loss 0.009474 train_mse 0.005047 train_perp 288.970540 valid_loss 0.081128 valid_mae 0.076089 valid_perp 69.394159 lr 0.000100\n",
      "Epoch 74 train_loss 0.009456 train_mse 0.005032 train_perp 289.920426 valid_loss 0.080875 valid_mae 0.075841 valid_perp 69.619445 lr 0.000100\n",
      "Epoch 75 train_loss 0.009430 train_mse 0.005013 train_perp 290.951065 valid_loss 0.080500 valid_mae 0.075467 valid_perp 69.817923 lr 0.000100\n",
      "Epoch 76 train_loss 0.009414 train_mse 0.004999 train_perp 292.191162 valid_loss 0.080753 valid_mae 0.075709 valid_perp 69.795841 lr 0.000100\n",
      "Epoch 77 train_loss 0.009389 train_mse 0.004980 train_perp 293.199068 valid_loss 0.080636 valid_mae 0.075600 valid_perp 69.817439 lr 0.000100\n",
      "Epoch 78 train_loss 0.009372 train_mse 0.004966 train_perp 294.053437 valid_loss 0.080649 valid_mae 0.075612 valid_perp 69.893407 lr 0.000100\n",
      "Epoch 79 train_loss 0.009347 train_mse 0.004948 train_perp 294.791301 valid_loss 0.080799 valid_mae 0.075767 valid_perp 69.899140 lr 0.000100\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 80 train_loss 0.009321 train_mse 0.004930 train_perp 295.547962 valid_loss 0.080371 valid_mae 0.075358 valid_perp 69.726144 lr 0.000010\n",
      "Epoch 81 train_loss 0.009291 train_mse 0.004914 train_perp 296.615302 valid_loss 0.080312 valid_mae 0.075309 valid_perp 70.047963 lr 0.000010\n",
      "Epoch 82 train_loss 0.009264 train_mse 0.004898 train_perp 297.831358 valid_loss 0.080193 valid_mae 0.075194 valid_perp 70.148156 lr 0.000010\n",
      "Epoch 83 train_loss 0.009252 train_mse 0.004890 train_perp 298.777125 valid_loss 0.080182 valid_mae 0.075189 valid_perp 70.217380 lr 0.000010\n",
      "Epoch 84 train_loss 0.009235 train_mse 0.004879 train_perp 299.584710 valid_loss 0.079973 valid_mae 0.074997 valid_perp 70.222700 lr 0.000010\n",
      "Epoch 85 train_loss 0.009212 train_mse 0.004866 train_perp 300.475648 valid_loss 0.079881 valid_mae 0.074907 valid_perp 70.265749 lr 0.000010\n",
      "Epoch 86 train_loss 0.009193 train_mse 0.004854 train_perp 301.218264 valid_loss 0.079774 valid_mae 0.074809 valid_perp 70.357628 lr 0.000010\n",
      "Epoch 87 train_loss 0.009174 train_mse 0.004842 train_perp 301.852181 valid_loss 0.079531 valid_mae 0.074590 valid_perp 70.557414 lr 0.000010\n",
      "Epoch 88 train_loss 0.009145 train_mse 0.004826 train_perp 302.473970 valid_loss 0.079421 valid_mae 0.074495 valid_perp 70.551263 lr 0.000010\n",
      "Epoch 89 train_loss 0.009127 train_mse 0.004815 train_perp 303.134001 valid_loss 0.079418 valid_mae 0.074492 valid_perp 70.547923 lr 0.000010\n",
      "Epoch 90 train_loss 0.009110 train_mse 0.004804 train_perp 304.006733 valid_loss 0.079441 valid_mae 0.074522 valid_perp 70.612010 lr 0.000010\n",
      "Epoch 91 train_loss 0.009087 train_mse 0.004790 train_perp 305.405890 valid_loss 0.079249 valid_mae 0.074343 valid_perp 70.653327 lr 0.000010\n",
      "Epoch 92 train_loss 0.009061 train_mse 0.004774 train_perp 306.590202 valid_loss 0.079130 valid_mae 0.074228 valid_perp 70.698382 lr 0.000010\n",
      "Epoch 93 train_loss 0.009036 train_mse 0.004759 train_perp 307.622383 valid_loss 0.079150 valid_mae 0.074249 valid_perp 70.838116 lr 0.000010\n",
      "Epoch 94 train_loss 0.009018 train_mse 0.004747 train_perp 308.548674 valid_loss 0.078986 valid_mae 0.074086 valid_perp 70.985767 lr 0.000010\n",
      "Epoch 95 train_loss 0.009011 train_mse 0.004741 train_perp 309.461085 valid_loss 0.078903 valid_mae 0.074012 valid_perp 71.200841 lr 0.000010\n",
      "Epoch 96 train_loss 0.008995 train_mse 0.004730 train_perp 310.190265 valid_loss 0.078897 valid_mae 0.074000 valid_perp 71.267099 lr 0.000010\n",
      "Epoch 97 train_loss 0.008979 train_mse 0.004720 train_perp 310.729234 valid_loss 0.078853 valid_mae 0.073956 valid_perp 71.550811 lr 0.000010\n",
      "Epoch 98 train_loss 0.008963 train_mse 0.004710 train_perp 311.421981 valid_loss 0.078956 valid_mae 0.074051 valid_perp 71.697434 lr 0.000010\n",
      "Epoch 99 train_loss 0.008951 train_mse 0.004702 train_perp 311.810892 valid_loss 0.078939 valid_mae 0.074030 valid_perp 71.694897 lr 0.000010\n",
      "Epoch 100 train_loss 0.008936 train_mse 0.004692 train_perp 312.205274 valid_loss 0.078779 valid_mae 0.073874 valid_perp 71.764940 lr 0.000010\n",
      "Epoch 101 train_loss 0.008922 train_mse 0.004683 train_perp 312.826667 valid_loss 0.078797 valid_mae 0.073900 valid_perp 71.811531 lr 0.000010\n",
      "Epoch 102 train_loss 0.008905 train_mse 0.004672 train_perp 313.263362 valid_loss 0.078824 valid_mae 0.073923 valid_perp 71.938092 lr 0.000010\n",
      "Epoch 103 train_loss 0.008898 train_mse 0.004666 train_perp 313.779743 valid_loss 0.078672 valid_mae 0.073779 valid_perp 72.003970 lr 0.000010\n",
      "Epoch 104 train_loss 0.008888 train_mse 0.004658 train_perp 314.129525 valid_loss 0.078626 valid_mae 0.073740 valid_perp 72.039402 lr 0.000010\n",
      "Epoch 105 train_loss 0.008882 train_mse 0.004652 train_perp 314.619246 valid_loss 0.078589 valid_mae 0.073707 valid_perp 72.072562 lr 0.000010\n",
      "Epoch 106 train_loss 0.008871 train_mse 0.004645 train_perp 314.867558 valid_loss 0.078627 valid_mae 0.073747 valid_perp 72.136898 lr 0.000010\n",
      "Epoch 107 train_loss 0.008860 train_mse 0.004637 train_perp 315.347874 valid_loss 0.078570 valid_mae 0.073685 valid_perp 72.412126 lr 0.000010\n",
      "Epoch 108 train_loss 0.008848 train_mse 0.004629 train_perp 315.921624 valid_loss 0.078493 valid_mae 0.073614 valid_perp 72.515268 lr 0.000010\n",
      "Epoch 109 train_loss 0.008837 train_mse 0.004622 train_perp 316.426508 valid_loss 0.078562 valid_mae 0.073686 valid_perp 72.596500 lr 0.000010\n",
      "Epoch 110 train_loss 0.008824 train_mse 0.004614 train_perp 316.745851 valid_loss 0.078351 valid_mae 0.073477 valid_perp 72.701532 lr 0.000010\n",
      "Epoch 111 train_loss 0.008815 train_mse 0.004608 train_perp 317.310215 valid_loss 0.078332 valid_mae 0.073457 valid_perp 72.860600 lr 0.000010\n",
      "Epoch 112 train_loss 0.008796 train_mse 0.004596 train_perp 317.998744 valid_loss 0.078222 valid_mae 0.073356 valid_perp 72.931413 lr 0.000010\n",
      "Epoch 113 train_loss 0.008784 train_mse 0.004588 train_perp 318.932470 valid_loss 0.078195 valid_mae 0.073326 valid_perp 73.126156 lr 0.000010\n",
      "Epoch 114 train_loss 0.008772 train_mse 0.004581 train_perp 320.011612 valid_loss 0.078132 valid_mae 0.073270 valid_perp 73.315661 lr 0.000010\n",
      "Epoch 115 train_loss 0.008758 train_mse 0.004572 train_perp 320.831012 valid_loss 0.078092 valid_mae 0.073234 valid_perp 73.379506 lr 0.000010\n",
      "Epoch 116 train_loss 0.008734 train_mse 0.004558 train_perp 321.492751 valid_loss 0.077888 valid_mae 0.073044 valid_perp 73.311838 lr 0.000010\n",
      "Epoch 117 train_loss 0.008715 train_mse 0.004548 train_perp 322.247883 valid_loss 0.077914 valid_mae 0.073076 valid_perp 73.457216 lr 0.000010\n",
      "Epoch 118 train_loss 0.008695 train_mse 0.004536 train_perp 323.155246 valid_loss 0.077762 valid_mae 0.072931 valid_perp 73.567370 lr 0.000010\n",
      "Epoch 119 train_loss 0.008684 train_mse 0.004528 train_perp 324.078500 valid_loss 0.077680 valid_mae 0.072852 valid_perp 73.590417 lr 0.000010\n",
      "Epoch 120 train_loss 0.008672 train_mse 0.004521 train_perp 324.802537 valid_loss 0.077658 valid_mae 0.072830 valid_perp 73.782935 lr 0.000010\n",
      "Epoch 121 train_loss 0.008660 train_mse 0.004513 train_perp 325.567898 valid_loss 0.077543 valid_mae 0.072721 valid_perp 73.793755 lr 0.000010\n",
      "Epoch 122 train_loss 0.008645 train_mse 0.004504 train_perp 326.091197 valid_loss 0.077547 valid_mae 0.072723 valid_perp 73.908090 lr 0.000010\n",
      "Epoch 123 train_loss 0.008642 train_mse 0.004501 train_perp 326.848077 valid_loss 0.077530 valid_mae 0.072710 valid_perp 73.891001 lr 0.000010\n",
      "Epoch 124 train_loss 0.008634 train_mse 0.004495 train_perp 327.322920 valid_loss 0.077505 valid_mae 0.072690 valid_perp 73.927813 lr 0.000010\n",
      "Epoch 125 train_loss 0.008624 train_mse 0.004489 train_perp 327.947999 valid_loss 0.077380 valid_mae 0.072571 valid_perp 74.101294 lr 0.000010\n",
      "Epoch 126 train_loss 0.008607 train_mse 0.004479 train_perp 328.397336 valid_loss 0.077332 valid_mae 0.072521 valid_perp 74.218521 lr 0.000010\n",
      "Epoch 127 train_loss 0.008595 train_mse 0.004471 train_perp 329.253290 valid_loss 0.077302 valid_mae 0.072497 valid_perp 74.376760 lr 0.000010\n",
      "Epoch 128 train_loss 0.008583 train_mse 0.004464 train_perp 329.896337 valid_loss 0.077173 valid_mae 0.072379 valid_perp 74.494620 lr 0.000010\n",
      "Epoch 129 train_loss 0.008576 train_mse 0.004458 train_perp 330.280440 valid_loss 0.077093 valid_mae 0.072301 valid_perp 74.475957 lr 0.000010\n",
      "Epoch 130 train_loss 0.008566 train_mse 0.004452 train_perp 330.843840 valid_loss 0.077002 valid_mae 0.072221 valid_perp 74.624574 lr 0.000010\n",
      "Epoch 131 train_loss 0.008557 train_mse 0.004446 train_perp 331.203929 valid_loss 0.076964 valid_mae 0.072186 valid_perp 74.690793 lr 0.000010\n",
      "Epoch 132 train_loss 0.008546 train_mse 0.004439 train_perp 331.422952 valid_loss 0.076952 valid_mae 0.072179 valid_perp 74.850984 lr 0.000010\n",
      "Epoch 133 train_loss 0.008539 train_mse 0.004434 train_perp 331.785862 valid_loss 0.076803 valid_mae 0.072042 valid_perp 74.968976 lr 0.000010\n",
      "Epoch 134 train_loss 0.008537 train_mse 0.004431 train_perp 332.526478 valid_loss 0.076751 valid_mae 0.071996 valid_perp 75.049632 lr 0.000010\n",
      "Epoch 135 train_loss 0.008520 train_mse 0.004422 train_perp 333.160247 valid_loss 0.076790 valid_mae 0.072030 valid_perp 75.268194 lr 0.000010\n",
      "Epoch 136 train_loss 0.008507 train_mse 0.004414 train_perp 333.767931 valid_loss 0.076745 valid_mae 0.071978 valid_perp 75.398354 lr 0.000010\n",
      "Epoch 137 train_loss 0.008503 train_mse 0.004410 train_perp 334.483078 valid_loss 0.076830 valid_mae 0.072070 valid_perp 75.526942 lr 0.000010\n",
      "Epoch 138 train_loss 0.008497 train_mse 0.004406 train_perp 335.000710 valid_loss 0.076656 valid_mae 0.071893 valid_perp 75.472572 lr 0.000010\n",
      "Epoch 139 train_loss 0.008489 train_mse 0.004401 train_perp 335.626068 valid_loss 0.076715 valid_mae 0.071954 valid_perp 75.480087 lr 0.000010\n",
      "Epoch 140 train_loss 0.008488 train_mse 0.004398 train_perp 335.955998 valid_loss 0.076654 valid_mae 0.071902 valid_perp 75.551248 lr 0.000010\n",
      "Epoch 141 train_loss 0.008473 train_mse 0.004390 train_perp 336.398764 valid_loss 0.076599 valid_mae 0.071842 valid_perp 75.589820 lr 0.000010\n",
      "Epoch 142 train_loss 0.008468 train_mse 0.004386 train_perp 336.815624 valid_loss 0.076667 valid_mae 0.071905 valid_perp 75.646116 lr 0.000010\n",
      "Epoch 143 train_loss 0.008466 train_mse 0.004383 train_perp 336.958396 valid_loss 0.076603 valid_mae 0.071839 valid_perp 75.633907 lr 0.000010\n",
      "Epoch 144 train_loss 0.008459 train_mse 0.004378 train_perp 337.210865 valid_loss 0.076704 valid_mae 0.071933 valid_perp 75.593005 lr 0.000010\n",
      "Epoch 145 train_loss 0.008462 train_mse 0.004378 train_perp 337.434895 valid_loss 0.076721 valid_mae 0.071950 valid_perp 75.683459 lr 0.000010\n",
      "Epoch   146: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 146 train_loss 0.008458 train_mse 0.004375 train_perp 337.594606 valid_loss 0.076656 valid_mae 0.071884 valid_perp 75.722972 lr 0.000001\n",
      "Epoch 147 train_loss 0.008448 train_mse 0.004370 train_perp 337.748393 valid_loss 0.076623 valid_mae 0.071856 valid_perp 75.795045 lr 0.000001\n",
      "Epoch 148 train_loss 0.008443 train_mse 0.004367 train_perp 337.951179 valid_loss 0.076550 valid_mae 0.071786 valid_perp 75.894412 lr 0.000001\n",
      "Epoch 149 train_loss 0.008434 train_mse 0.004362 train_perp 338.159379 valid_loss 0.076492 valid_mae 0.071738 valid_perp 75.804066 lr 0.000001\n",
      "Epoch 150 train_loss 0.008423 train_mse 0.004356 train_perp 338.366151 valid_loss 0.076399 valid_mae 0.071650 valid_perp 75.733133 lr 0.000001\n",
      "Epoch 151 train_loss 0.008410 train_mse 0.004349 train_perp 338.482704 valid_loss 0.076327 valid_mae 0.071583 valid_perp 75.549733 lr 0.000001\n",
      "Epoch 152 train_loss 0.008399 train_mse 0.004344 train_perp 338.576202 valid_loss 0.076290 valid_mae 0.071552 valid_perp 75.476026 lr 0.000001\n",
      "Epoch 153 train_loss 0.008392 train_mse 0.004340 train_perp 338.746366 valid_loss 0.076257 valid_mae 0.071516 valid_perp 75.375596 lr 0.000001\n",
      "Epoch 154 train_loss 0.008387 train_mse 0.004337 train_perp 338.789117 valid_loss 0.076228 valid_mae 0.071488 valid_perp 75.482069 lr 0.000001\n",
      "Epoch 155 train_loss 0.008382 train_mse 0.004334 train_perp 338.860361 valid_loss 0.076244 valid_mae 0.071502 valid_perp 75.509509 lr 0.000001\n",
      "Epoch 156 train_loss 0.008375 train_mse 0.004330 train_perp 339.050540 valid_loss 0.076221 valid_mae 0.071479 valid_perp 75.459620 lr 0.000001\n",
      "Epoch 157 train_loss 0.008368 train_mse 0.004326 train_perp 339.097006 valid_loss 0.076224 valid_mae 0.071482 valid_perp 75.408725 lr 0.000001\n",
      "Epoch 158 train_loss 0.008362 train_mse 0.004323 train_perp 339.221441 valid_loss 0.076189 valid_mae 0.071445 valid_perp 75.411803 lr 0.000001\n",
      "Epoch 159 train_loss 0.008356 train_mse 0.004319 train_perp 339.129977 valid_loss 0.076183 valid_mae 0.071443 valid_perp 75.340195 lr 0.000001\n",
      "Epoch 160 train_loss 0.008355 train_mse 0.004318 train_perp 339.288997 valid_loss 0.076177 valid_mae 0.071436 valid_perp 75.455889 lr 0.000001\n",
      "Epoch 161 train_loss 0.008348 train_mse 0.004315 train_perp 339.400134 valid_loss 0.076146 valid_mae 0.071407 valid_perp 75.402539 lr 0.000001\n",
      "Epoch 162 train_loss 0.008346 train_mse 0.004314 train_perp 339.353264 valid_loss 0.076081 valid_mae 0.071352 valid_perp 75.482077 lr 0.000001\n",
      "Epoch 163 train_loss 0.008342 train_mse 0.004311 train_perp 339.498149 valid_loss 0.076024 valid_mae 0.071302 valid_perp 75.388463 lr 0.000001\n",
      "Epoch 164 train_loss 0.008335 train_mse 0.004308 train_perp 339.537031 valid_loss 0.076080 valid_mae 0.071354 valid_perp 75.393522 lr 0.000001\n",
      "Epoch 165 train_loss 0.008330 train_mse 0.004305 train_perp 339.581833 valid_loss 0.075997 valid_mae 0.071278 valid_perp 75.398751 lr 0.000001\n",
      "Epoch 166 train_loss 0.008323 train_mse 0.004301 train_perp 339.737882 valid_loss 0.075963 valid_mae 0.071245 valid_perp 75.388831 lr 0.000001\n",
      "Epoch 167 train_loss 0.008323 train_mse 0.004301 train_perp 339.980044 valid_loss 0.075939 valid_mae 0.071228 valid_perp 75.381584 lr 0.000001\n",
      "Epoch 168 train_loss 0.008312 train_mse 0.004295 train_perp 339.967035 valid_loss 0.075943 valid_mae 0.071227 valid_perp 75.489276 lr 0.000001\n",
      "Epoch 169 train_loss 0.008305 train_mse 0.004291 train_perp 340.065327 valid_loss 0.075919 valid_mae 0.071198 valid_perp 75.529209 lr 0.000001\n",
      "Epoch 170 train_loss 0.008300 train_mse 0.004288 train_perp 340.286737 valid_loss 0.075911 valid_mae 0.071190 valid_perp 75.493368 lr 0.000001\n",
      "Epoch 171 train_loss 0.008303 train_mse 0.004289 train_perp 340.318128 valid_loss 0.075883 valid_mae 0.071164 valid_perp 75.584046 lr 0.000001\n",
      "Epoch 172 train_loss 0.008292 train_mse 0.004283 train_perp 340.597654 valid_loss 0.075881 valid_mae 0.071160 valid_perp 75.564708 lr 0.000001\n",
      "Epoch 173 train_loss 0.008289 train_mse 0.004281 train_perp 341.036555 valid_loss 0.075916 valid_mae 0.071191 valid_perp 75.740720 lr 0.000001\n",
      "Epoch 174 train_loss 0.008284 train_mse 0.004278 train_perp 341.361175 valid_loss 0.075941 valid_mae 0.071217 valid_perp 75.824481 lr 0.000001\n",
      "Epoch 175 train_loss 0.008284 train_mse 0.004278 train_perp 341.678287 valid_loss 0.075949 valid_mae 0.071222 valid_perp 75.807909 lr 0.000001\n",
      "Epoch   176: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 176 train_loss 0.008283 train_mse 0.004277 train_perp 341.844738 valid_loss 0.075954 valid_mae 0.071228 valid_perp 75.910456 lr 0.000000\n",
      "Epoch 177 train_loss 0.008277 train_mse 0.004274 train_perp 341.943161 valid_loss 0.075949 valid_mae 0.071227 valid_perp 75.919597 lr 0.000000\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    # Train in batch mode\n",
    "    vae_model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_perp = []\n",
    "    train_mse = []\n",
    "    \n",
    "    for k, (x, y) in enumerate(train_loader):\n",
    "        vq_loss, x_pred, perplexity = vae_model(x.to(device))\n",
    "        #recon_loss = F.mse_loss(y.to(device), y_pred) #/ data_variance\n",
    "        recon_loss = torch.nn.HuberLoss()(x[:, out_col_idx].to(device), x_pred) \n",
    "        loss = recon_loss + loss_weight * vq_loss\n",
    "        \n",
    "#         l1_norm = sum(p.abs().sum() for p in vae_model.parameters())\n",
    "#         l2_norm = sum(p.pow(2.0).sum() for p in vae_model.parameters())\n",
    "        \n",
    "#         loss += L1_penalty * l1_norm\n",
    "#         loss += L2_penalty * l2_norm\n",
    "               \n",
    "        train_loss.append(loss.item())\n",
    "        train_mse.append(recon_loss.item())\n",
    "        train_perp.append(perplexity.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #lr_scheduler.step()\n",
    "\n",
    "    # Validate \n",
    "    vae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Validate in batch mode\n",
    "        valid_loss = []\n",
    "        valid_perp = []\n",
    "        valid_mse = []\n",
    "        \n",
    "        for k, (x, y) in enumerate(valid_loader):\n",
    "            vq_loss, x_pred, perplexity = vae_model(x.to(device))\n",
    "            #recon_loss = F.mse_loss(y.to(device), y_pred) #/ data_variance\n",
    "            recon_loss = torch.nn.L1Loss()(x[:, out_col_idx].to(device), x_pred)\n",
    "            loss = recon_loss + vq_loss\n",
    "        \n",
    "            valid_loss.append(loss.item())\n",
    "            valid_mse.append(recon_loss.item())\n",
    "            valid_perp.append(perplexity.item())\n",
    "        \n",
    "    results_dict[\"epoch\"].append(epoch)\n",
    "    results_dict[\"train_loss\"].append(np.mean(train_loss))\n",
    "    results_dict[\"train_perp\"].append(np.mean(train_perp))\n",
    "    results_dict[\"train_mse\"].append(np.mean(train_mse))\n",
    "    results_dict[\"valid_loss\"].append(np.mean(valid_loss))\n",
    "    results_dict[\"valid_perp\"].append(np.mean(valid_perp))\n",
    "    results_dict[\"valid_mae\"].append(np.mean(valid_mse))\n",
    "    results_dict[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save the dataframe to disk\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "    df.to_csv(f\"gecko/training_log.csv\", index = False)\n",
    "    \n",
    "    print(f'Epoch {epoch}',\n",
    "          f'train_loss {results_dict[\"train_loss\"][-1]:2f}',\n",
    "          f'train_mse {results_dict[\"train_mse\"][-1]:2f}',\n",
    "          f'train_perp {results_dict[\"train_perp\"][-1]:2f}',\n",
    "          f'valid_loss {results_dict[\"valid_loss\"][-1]:2f}',\n",
    "          f'valid_mae {results_dict[\"valid_mae\"][-1]:2f}',\n",
    "          f'valid_perp {results_dict[\"valid_perp\"][-1]:2f}',\n",
    "          f'lr {results_dict[\"lr\"][-1]:6f}'\n",
    "         )\n",
    "\n",
    "    # anneal the learning rate using just the box metric\n",
    "    lr_scheduler.step(results_dict[\"valid_mae\"][-1])\n",
    "    \n",
    "    if results_dict[\"valid_mae\"][-1] == min(results_dict[\"valid_mae\"]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': min(results_dict[\"valid_mae\"])\n",
    "        }\n",
    "        torch.save(state_dict, f\"gecko/vae.pt\")\n",
    "    \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(results_dict[\"valid_mae\"]) if j == min(results_dict[\"valid_mae\"])][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "pred_x = vae_model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.9863,  0.8765, -0.5707],\n",
       "         [ 0.9863,  0.8765, -0.5707],\n",
       "         [ 0.9863,  0.8765, -0.5707],\n",
       "         ...,\n",
       "         [-0.1751, -0.8979, -0.5781],\n",
       "         [-0.1751, -0.8979, -0.5781],\n",
       "         [-0.1751, -0.8979, -0.5781]], device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " tensor(13.1072, device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8855,  0.9361, -0.5744],\n",
       "        [ 0.8845,  0.9349, -0.5743],\n",
       "        [ 0.8835,  0.9336, -0.5742],\n",
       "        ...,\n",
       "        [-0.1712, -0.8818, -0.5606],\n",
       "        [-0.1722, -0.8829, -0.5606],\n",
       "        [-0.1731, -0.8840, -0.5605]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, out_col_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size  = 100, output_size = 3, fcl_layers = 1, dr = 0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fcn = self.make_fcn(input_size, output_size, fcl_layers, dr)\n",
    "\n",
    "    def make_fcn(self, input_size, output_size, fcl_layers, dr):\n",
    "        if len(fcl_layers) > 0:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, fcl_layers[0]),\n",
    "                #nn.BatchNorm1d(fcl_layers[0]),\n",
    "                torch.nn.ReLU(),\n",
    "                #nn.Dropout(dr)\n",
    "            ]\n",
    "            if len(fcl_layers) == 1:\n",
    "                fcn.append(nn.Linear(fcl_layers[0], output_size))\n",
    "            else:\n",
    "                for i in range(len(fcl_layers)-1):\n",
    "                    fcn += [\n",
    "                        nn.Linear(fcl_layers[i], fcl_layers[i+1]),\n",
    "                        #nn.BatchNorm1d(fcl_layers[i+1]),\n",
    "                        torch.nn.ReLU(),\n",
    "                        #nn.Dropout(dr)\n",
    "                    ]\n",
    "                fcn.append(nn.Linear(fcl_layers[i+1], output_size))\n",
    "        else:\n",
    "            fcn = [\n",
    "                nn.Linear(input_size, output_size)\n",
    "            ]\n",
    "        return nn.Sequential(*fcn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fcn(x)\n",
    "        return x\n",
    "\n",
    "class box_vqvae(nn.Module):\n",
    "    def __init__(self, input_size, middle_size, output_size, fcl_layers, dr,\n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        \n",
    "        super(box_vqvae, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(input_size, embedding_dim, fcl_layers, dr)\n",
    "        \n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "            \n",
    "        self._decoder = Decoder(embedding_dim, output_size, fcl_layers[::-1][1:], dr)\n",
    "        \n",
    "#         self.fc1 = torch.nn.Linear(input_size + output_size, middle_size)\n",
    "#         #self.bn1 = nn.BatchNorm1d(middle_size)\n",
    "#         self.ac1 = nn.ReLU()\n",
    "#         #self.dr1 = nn.Dropout(dr)\n",
    "#         self.fc2 = torch.nn.Linear(middle_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "#         x = torch.cat([x, x_recon], axis = 1)\n",
    "#         x = self.fc1(x)\n",
    "#         #x = self.bn1(x)\n",
    "#         x = self.ac1(x)\n",
    "#         #x = self.dr1(x)\n",
    "#         x = self.fc2(x)\n",
    "        return loss, x_recon, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 1024\n",
    "embedding_dim = 512\n",
    "fcl_layers = []\n",
    "\n",
    "integrator = box_vqvae(input_size, middle_size, 3, \n",
    "              fcl_layers, dr, num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "\n",
    "integrator = integrator.apply(initialize_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "box_vqvae(\n",
       "  (_encoder): Encoder(\n",
       "    (fcn): Sequential(\n",
       "      (0): Linear(in_features=9, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_vq_vae): VectorQuantizerEMA(\n",
       "    (_embedding): Embedding(1024, 512)\n",
       "  )\n",
       "  (_decoder): Decoder(\n",
       "    (fcn): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(integrator.parameters(),\n",
    "                             lr = 1e-3,\n",
    "                             eps = 1e-7,\n",
    "                             betas = (0.9, 0.999),\n",
    "                             amsgrad = False)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer, \n",
    "#     T_max=10, \n",
    "#     eta_min=1e-3*learning_rate\n",
    "# )\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        patience = lr_patience, \n",
    "        verbose = True,\n",
    "        min_lr = 1.0e-13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60><function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60><function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "            self._shutdown_workers()self._shutdown_workers()\n",
      "self._shutdown_workers()\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "            if w.is_alive():if w.is_alive():if w.is_alive():\n",
      "\n",
      "\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "        assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionErrorAssertionError: can only test a child process: can only test a child process\n",
      "\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>Traceback (most recent call last):\n",
      "\n",
      "\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "self._shutdown_workers()  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "      File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "self._shutdown_workers()        \n",
      "self._shutdown_workers()    self._shutdown_workers()  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "if w.is_alive():\n",
      "\n",
      "\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x2aec7627de60>      File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "if w.is_alive():            if w.is_alive():\n",
      "if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "AssertionError        :     assert self._parent_pid == os.getpid(), 'can only test a child process'assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "\n",
      "\n",
      "AssertionErrorAssertionErrorAssertionError: can only test a child process: \n",
      "can only test a child process\n",
      "Traceback (most recent call last):\n",
      ":   File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "can only test a child process    \n",
      "self._shutdown_workers()\n",
      "  File \"/glade/work/schreck/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_loss 0.145813 train_mse 0.081443 train_perp 113.754626 valid_loss 0.103038 valid_mae 0.095279 valid_perp 3.945969 n 0 lr 0.001000\n",
      "Epoch 1 train_loss 0.010103 train_mse 0.005799 train_perp 157.658785 valid_loss 0.087850 valid_mae 0.083142 valid_perp 4.853414 n 0 lr 0.001000\n",
      "Epoch 2 train_loss 0.008979 train_mse 0.005048 train_perp 154.722582 valid_loss 0.080108 valid_mae 0.075404 valid_perp 4.798885 n 0 lr 0.001000\n",
      "Epoch 3 train_loss 0.008774 train_mse 0.004850 train_perp 151.375798 valid_loss 0.081043 valid_mae 0.076317 valid_perp 4.685005 n 0 lr 0.001000\n",
      "Epoch 4 train_loss 0.008772 train_mse 0.004800 train_perp 149.141848 valid_loss 0.085481 valid_mae 0.080618 valid_perp 4.640080 n 0 lr 0.001000\n",
      "Epoch 5 train_loss 0.008781 train_mse 0.004780 train_perp 145.699964 valid_loss 0.082374 valid_mae 0.077513 valid_perp 4.574078 n 0 lr 0.001000\n",
      "Epoch 6 train_loss 0.008821 train_mse 0.004785 train_perp 143.498911 valid_loss 0.082138 valid_mae 0.077234 valid_perp 4.540907 n 0 lr 0.001000\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 7 train_loss 0.008341 train_mse 0.004217 train_perp 143.278224 valid_loss 0.079214 valid_mae 0.074078 valid_perp 4.558657 n 0 lr 0.000100\n",
      "Epoch 8 train_loss 0.008490 train_mse 0.004219 train_perp 142.722810 valid_loss 0.078945 valid_mae 0.073683 valid_perp 4.563396 n 0 lr 0.000100\n",
      "Epoch 9 train_loss 0.008614 train_mse 0.004228 train_perp 142.514804 valid_loss 0.079741 valid_mae 0.074320 valid_perp 4.533104 n 0 lr 0.000100\n",
      "Epoch 10 train_loss 0.008708 train_mse 0.004242 train_perp 141.711605 valid_loss 0.078682 valid_mae 0.073205 valid_perp 4.558425 n 0 lr 0.000100\n",
      "Epoch 11 train_loss 0.008791 train_mse 0.004260 train_perp 140.732790 valid_loss 0.079572 valid_mae 0.074064 valid_perp 4.590193 n 0 lr 0.000100\n",
      "Epoch 12 train_loss 0.008826 train_mse 0.004258 train_perp 140.884653 valid_loss 0.079725 valid_mae 0.074158 valid_perp 4.600397 n 0 lr 0.000100\n",
      "Epoch 13 train_loss 0.008801 train_mse 0.004237 train_perp 141.356317 valid_loss 0.080708 valid_mae 0.075077 valid_perp 4.573403 n 0 lr 0.000100\n",
      "Epoch 14 train_loss 0.008822 train_mse 0.004236 train_perp 141.879537 valid_loss 0.080164 valid_mae 0.074524 valid_perp 4.589585 n 0 lr 0.000100\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 15 train_loss 0.008756 train_mse 0.004140 train_perp 142.396734 valid_loss 0.078803 valid_mae 0.073128 valid_perp 4.622910 n 0 lr 0.000010\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    # Train in batch mode\n",
    "    integrator.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_perp = []\n",
    "    train_mse = []\n",
    "    \n",
    "    for k, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "        vq_loss, y_pred, perplexity = integrator(x.to(device))\n",
    "        #recon_loss = F.mse_loss(y.to(device), y_pred) #/ data_variance\n",
    "        recon_loss = torch.nn.HuberLoss()(y.to(device), y_pred) \n",
    "        loss = recon_loss + loss_weight * vq_loss\n",
    "        \n",
    "#         l1_norm = sum(p.abs().sum() for p in integrator.parameters())\n",
    "#         l2_norm = sum(p.pow(2.0).sum() for p in integrator.parameters())\n",
    "        \n",
    "#         loss += L1_penalty * l1_norm\n",
    "#         loss += L2_penalty * l2_norm\n",
    "               \n",
    "        train_loss.append(loss.item())\n",
    "        train_mse.append(recon_loss.item())\n",
    "        train_perp.append(perplexity.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    # Train in box mode\n",
    "#     integrator.train()\n",
    "    \n",
    "#     train_loss = []\n",
    "#     train_perp = []\n",
    "#     train_mse = []\n",
    "\n",
    "#     # set up array for saving predicted results\n",
    "#     _in_array = torch.from_numpy(train_in_array).to(device).float()\n",
    "#     #pred_array = np.empty((train_in_array.shape[0], num_timesteps-start_time, len(out_col_idx)))\n",
    "\n",
    "#     # use initial condition @ t = start_time and get the first prediction\n",
    "#     vq_loss, output, perplexity = integrator(_in_array[:, start_time, :].to(device))\n",
    "#     #pred_array[:, 0, :] = output.cpu().numpy()\n",
    "#     recon_loss = F.l1_loss(_in_array[:, start_time + 1, out_col_idx], output)\n",
    "#     loss = recon_loss + loss_weight * vq_loss\n",
    "#     l1_norm = sum(p.abs().sum() for p in integrator.parameters()).cpu()\n",
    "#     loss += L1_penalty * l1_norm\n",
    "    \n",
    "#     train_loss.append(loss.item())\n",
    "#     train_mse.append(recon_loss.item())\n",
    "#     train_perp.append(perplexity.item())\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # use the first prediction to get the next, and so on for num_timesteps\n",
    "#     for k, i in enumerate(range(start_time + 1, num_timesteps)): \n",
    "#         new_input = _in_array[:, i, :]\n",
    "#         new_input[:, out_col_idx] = output.detach()\n",
    "#         vq_loss, output, perplexity = integrator(new_input)\n",
    "#         if i < (num_timesteps-1):\n",
    "#             recon_loss = F.l1_loss(_in_array[:, i + 1, out_col_idx], output)\n",
    "#             loss = recon_loss + loss_weight * vq_loss\n",
    "    \n",
    "#             train_loss.append(loss.item())\n",
    "#             train_mse.append(recon_loss.item())\n",
    "#             train_perp.append(perplexity.item())\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "    # Validate \n",
    "    integrator.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Validate in batch mode\n",
    "        valid_loss = []\n",
    "        valid_perp = []\n",
    "        valid_mse = []\n",
    "        \n",
    "        for k, (x, y) in enumerate(valid_loader):\n",
    "            \n",
    "            vq_loss, y_pred, perplexity = integrator(x.to(device))\n",
    "            \n",
    "            #recon_loss = F.mse_loss(y.to(device), y_pred) #/ data_variance\n",
    "            recon_loss = torch.nn.L1Loss()(y.to(device), y_pred) \n",
    "            loss = recon_loss + vq_loss\n",
    "        \n",
    "            valid_loss.append(loss.item())\n",
    "            valid_mse.append(recon_loss.item())\n",
    "            valid_perp.append(perplexity.item())\n",
    "            \n",
    "#         # Validate in box mode\n",
    "#         box_loss = []\n",
    "#         box_perp = []\n",
    "#         box_mse = []\n",
    "        \n",
    "#         # set up array for saving predicted results\n",
    "#         _in_array = torch.from_numpy(val_in_array).to(device).float()\n",
    "#         pred_array = np.empty((val_in_array.shape[0], num_timesteps-start_time, len(out_col_idx)))\n",
    "\n",
    "#         # use initial condition @ t = start_time and get the first prediction\n",
    "#         vq_loss, y_pred, perplexity = integrator(_in_array[:, start_time, :].to(device))\n",
    "#         pred_array[:, 0, :] = y_pred.cpu().numpy()\n",
    "#         recon_loss = F.l1_loss(_in_array[:, start_time + 1, out_col_idx], y_pred)\n",
    "#         loss = recon_loss + loss_weight * vq_loss\n",
    "        \n",
    "#         box_loss.append(loss.item())\n",
    "#         box_mse.append(recon_loss.item())\n",
    "#         box_perp.append(perplexity.item())\n",
    "\n",
    "#         # use the first prediction to get the next, and so on for num_timesteps\n",
    "#         for k, i in enumerate(range(start_time + 1, num_timesteps)): \n",
    "#             new_input = _in_array[:, i, :]\n",
    "#             new_input[:, out_col_idx] = y_pred\n",
    "#             vq_loss, y_pred, perplexity = integrator(new_input.to(device))\n",
    "#             pred_array[:, k+1, :] = y_pred.cpu().numpy()\n",
    "#             if i < (num_timesteps-1):\n",
    "#                 recon_loss = F.l1_loss(_in_array[:, i + 1, out_col_idx], y_pred)\n",
    "#                 loss = recon_loss + loss_weight * vq_loss\n",
    "#                 box_loss.append(loss.item())\n",
    "#                 box_mse.append(recon_loss.item())\n",
    "#                 box_perp.append(perplexity.item())\n",
    "                \n",
    "#         idx = transformed_data[\"val_out\"].index\n",
    "#         start_time_units = sorted(list(set([x[0] for x in idx])))[start_time]\n",
    "#         start_time_condition = [(x[0] >= start_time_units) for x in idx]\n",
    "#         idx = transformed_data[\"val_out\"][start_time_condition].index\n",
    "\n",
    "#         raw_box_preds = pd.DataFrame(\n",
    "#             data=pred_array.reshape(-1, len(output_vars)),\n",
    "#             columns=output_vars, \n",
    "#             index=idx\n",
    "#         )\n",
    "\n",
    "#         # inverse transform \n",
    "#         truth, preds = inv_transform_preds(\n",
    "#             raw_preds=raw_box_preds,\n",
    "#             truth=data['val_out'][start_time_condition],\n",
    "#             y_scaler=y_scaler,\n",
    "#             log_trans_cols=log_trans_cols,\n",
    "#             tendency_cols=tendency_cols)\n",
    "                \n",
    "#         metrics = ensembled_metrics(y_true=truth,\n",
    "#                                     y_pred=preds,\n",
    "#                                     member=0,\n",
    "#                                     output_vars=output_vars,\n",
    "#                                     stability_thresh=1.0)\n",
    "#         mean_box_mae = metrics['mean_mae'].mean()\n",
    "#         unstable_exps = int(metrics['n_unstable'].mean())\n",
    "        \n",
    "    results_dict[\"epoch\"].append(epoch)\n",
    "    results_dict[\"train_loss\"].append(np.mean(train_loss))\n",
    "    results_dict[\"train_perp\"].append(np.mean(train_perp))\n",
    "    results_dict[\"train_mse\"].append(np.mean(train_mse))\n",
    "    results_dict[\"valid_loss\"].append(np.mean(valid_loss))\n",
    "    results_dict[\"valid_perp\"].append(np.mean(valid_perp))\n",
    "    results_dict[\"valid_mae\"].append(np.mean(valid_mse))\n",
    "#     results_dict[\"box_loss\"].append(np.mean(box_loss))\n",
    "#     results_dict[\"box_perp\"].append(np.mean(box_perp))\n",
    "#     results_dict[\"box_mse\"].append(np.mean(box_mse))\n",
    "#     results_dict[\"box_mae\"].append(mean_box_mae)\n",
    "    results_dict[\"lr\"].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save the dataframe to disk\n",
    "    df = pd.DataFrame.from_dict(results_dict).reset_index()\n",
    "    df.to_csv(f\"gecko/integrator_training_log.csv\", index = False)\n",
    "    \n",
    "    print(f'Epoch {epoch}',\n",
    "          f'train_loss {results_dict[\"train_loss\"][-1]:2f}',\n",
    "          f'train_mse {results_dict[\"train_mse\"][-1]:2f}',\n",
    "          f'train_perp {results_dict[\"train_perp\"][-1]:2f}',\n",
    "          f'valid_loss {results_dict[\"valid_loss\"][-1]:2f}',\n",
    "          f'valid_mae {results_dict[\"valid_mae\"][-1]:2f}',\n",
    "          f'valid_perp {results_dict[\"valid_perp\"][-1]:2f}',\n",
    "#           f'box_loss {results_dict[\"box_loss\"][-1]:2f}',\n",
    "#           f'box_mse {results_dict[\"box_mse\"][-1]:2f}',\n",
    "#           f'box_perp {results_dict[\"box_perp\"][-1]:2f}',\n",
    "#           f'box_mae {results_dict[\"box_mae\"][-1]:2f}',\n",
    "          f'n {unstable_exps}',\n",
    "          f'lr {results_dict[\"lr\"][-1]:6f}'\n",
    "         )\n",
    "\n",
    "    # anneal the learning rate using just the box metric\n",
    "    lr_scheduler.step(results_dict[\"valid_loss\"][-1])\n",
    "    \n",
    "    if results_dict[\"valid_loss\"][-1] == min(results_dict[\"valid_loss\"]):\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': min(results_dict[\"valid_loss\"])\n",
    "        }\n",
    "        torch.save(state_dict, f\"gecko/integrator.pt\")\n",
    "    \n",
    "    # Stop training if we have not improved after X epochs\n",
    "    best_epoch = [i for i,j in enumerate(results_dict[\"valid_loss\"]) if j == min(results_dict[\"valid_loss\"])][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = integrator(x.to(device))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8845,  0.9349, -0.5743],\n",
       "        [ 0.8835,  0.9336, -0.5742],\n",
       "        [ 0.8825,  0.9324, -0.5742],\n",
       "        ...,\n",
       "        [-0.1722, -0.8829, -0.5606],\n",
       "        [-0.1731, -0.8840, -0.5605],\n",
       "        [-0.1741, -0.8851, -0.5605]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
